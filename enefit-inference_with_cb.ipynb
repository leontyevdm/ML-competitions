{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b7aa8f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:58:16.750038Z",
     "iopub.status.busy": "2024-01-31T19:58:16.749575Z",
     "iopub.status.idle": "2024-01-31T19:58:24.212660Z",
     "shell.execute_reply": "2024-01-31T19:58:24.211670Z"
    },
    "papermill": {
     "duration": 7.476671,
     "end_time": "2024-01-31T19:58:24.215309",
     "exception": false,
     "start_time": "2024-01-31T19:58:16.738638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e89add9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:58:24.236602Z",
     "iopub.status.busy": "2024-01-31T19:58:24.236221Z",
     "iopub.status.idle": "2024-01-31T19:58:24.259529Z",
     "shell.execute_reply": "2024-01-31T19:58:24.258350Z"
    },
    "papermill": {
     "duration": 0.03593,
     "end_time": "2024-01-31T19:58:24.261883",
     "exception": false,
     "start_time": "2024-01-31T19:58:24.225953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataStorage:\n",
    "    root = \"/kaggle/input/predict-energy-behavior-of-prosumers\"\n",
    "\n",
    "    data_cols = [\n",
    "        \"target\",\n",
    "        \"county\",\n",
    "        \"is_business\",\n",
    "        \"product_type\",\n",
    "        \"is_consumption\",\n",
    "        \"datetime\",\n",
    "        \"row_id\",\n",
    "    ]\n",
    "    client_cols = [\n",
    "        \"product_type\",\n",
    "        \"county\",\n",
    "        \"eic_count\",\n",
    "        \"installed_capacity\",\n",
    "        \"is_business\",\n",
    "        \"date\",\n",
    "    ]\n",
    "    forecast_weather_cols = [\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"hours_ahead\",\n",
    "        \"temperature\",\n",
    "        \"dewpoint\",\n",
    "        \"cloudcover_high\",\n",
    "        \"cloudcover_low\",\n",
    "        \"cloudcover_mid\",\n",
    "        \"cloudcover_total\",\n",
    "        \"10_metre_u_wind_component\",\n",
    "        \"10_metre_v_wind_component\",\n",
    "        \"forecast_datetime\",\n",
    "        \"direct_solar_radiation\",\n",
    "        \"surface_solar_radiation_downwards\",\n",
    "        \"snowfall\",\n",
    "        \"total_precipitation\",\n",
    "    ]\n",
    "    historical_weather_cols = [\n",
    "        \"datetime\",\n",
    "        \"temperature\",\n",
    "        \"dewpoint\",\n",
    "        \"rain\",\n",
    "        \"snowfall\",\n",
    "        \"surface_pressure\",\n",
    "        \"cloudcover_total\",\n",
    "        \"cloudcover_low\",\n",
    "        \"cloudcover_mid\",\n",
    "        \"cloudcover_high\",\n",
    "        \"windspeed_10m\",\n",
    "        \"winddirection_10m\",\n",
    "        \"shortwave_radiation\",\n",
    "        \"direct_solar_radiation\",\n",
    "        \"diffuse_radiation\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "    ]\n",
    "    location_cols = [\"longitude\", \"latitude\", \"county\"]\n",
    "    target_cols = [\n",
    "        \"target\",\n",
    "        \"county\",\n",
    "        \"is_business\",\n",
    "        \"product_type\",\n",
    "        \"is_consumption\",\n",
    "        \"datetime\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df_data = pl.read_csv(\n",
    "            os.path.join(self.root, \"train.csv\"),\n",
    "            columns=self.data_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_client = pl.read_csv(\n",
    "            os.path.join(self.root, \"client.csv\"),\n",
    "            columns=self.client_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_forecast_weather = pl.read_csv(\n",
    "            os.path.join(self.root, \"forecast_weather.csv\"),\n",
    "            columns=self.forecast_weather_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_historical_weather = pl.read_csv(\n",
    "            os.path.join(self.root, \"historical_weather.csv\"),\n",
    "            columns=self.historical_weather_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        self.df_weather_station_to_county_mapping = pl.read_csv(\n",
    "            os.path.join(self.root, \"weather_station_to_county_mapping.csv\"),\n",
    "            columns=self.location_cols,\n",
    "            try_parse_dates=True,\n",
    "        )\n",
    "        \n",
    "        self.df_target = self.df_data.select(self.target_cols)\n",
    "        \n",
    "\n",
    "\n",
    "        self.schema_data = self.df_data.schema\n",
    "        self.schema_client = self.df_client.schema\n",
    "        self.schema_forecast_weather = self.df_forecast_weather.schema\n",
    "        self.schema_historical_weather = self.df_historical_weather.schema\n",
    "        self.schema_target = self.df_target.schema\n",
    "\n",
    "        self.df_weather_station_to_county_mapping = (\n",
    "            self.df_weather_station_to_county_mapping.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def update_with_new_data(\n",
    "        self,\n",
    "        df_new_client,\n",
    "        df_new_forecast_weather,\n",
    "        df_new_historical_weather,\n",
    "        df_new_target\n",
    "    ):\n",
    "        df_new_client = pl.from_pandas(\n",
    "            df_new_client[self.client_cols], schema_overrides=self.schema_client\n",
    "        )\n",
    "        df_new_forecast_weather = pl.from_pandas(\n",
    "            df_new_forecast_weather[self.forecast_weather_cols],\n",
    "            schema_overrides=self.schema_forecast_weather,\n",
    "        )\n",
    "        df_new_historical_weather = pl.from_pandas(\n",
    "            df_new_historical_weather[self.historical_weather_cols],\n",
    "            schema_overrides=self.schema_historical_weather,\n",
    "        )\n",
    "        df_new_target = pl.from_pandas(\n",
    "            df_new_target[self.target_cols], schema_overrides=self.schema_target\n",
    "        )\n",
    "\n",
    "        self.df_client = pl.concat([self.df_client, df_new_client]).unique(\n",
    "            [\"date\", \"county\", \"is_business\", \"product_type\"]\n",
    "        )\n",
    "        self.df_forecast_weather = pl.concat(\n",
    "            [self.df_forecast_weather, df_new_forecast_weather]\n",
    "        ).unique([\"forecast_datetime\", \"latitude\", \"longitude\", \"hours_ahead\"])\n",
    "        self.df_historical_weather = pl.concat(\n",
    "            [self.df_historical_weather, df_new_historical_weather]\n",
    "        ).unique([\"datetime\", \"latitude\", \"longitude\"])\n",
    "        self.df_target = pl.concat([self.df_target, df_new_target]).unique(\n",
    "            [\"datetime\", \"county\", \"is_business\", \"product_type\", \"is_consumption\"]\n",
    "        )\n",
    "\n",
    "    def preprocess_test(self, df_test):\n",
    "        df_test = df_test.rename(columns={\"prediction_datetime\": \"datetime\"})\n",
    "        df_test = pl.from_pandas(\n",
    "            df_test[self.data_cols[1:]], schema_overrides=self.schema_data\n",
    "        )\n",
    "        return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b75f1b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:58:24.280872Z",
     "iopub.status.busy": "2024-01-31T19:58:24.280534Z",
     "iopub.status.idle": "2024-01-31T19:58:24.494938Z",
     "shell.execute_reply": "2024-01-31T19:58:24.493917Z"
    },
    "papermill": {
     "duration": 0.227128,
     "end_time": "2024-01-31T19:58:24.497562",
     "exception": false,
     "start_time": "2024-01-31T19:58:24.270434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "class FeaturesGeneratorConsumptionNormalized:\n",
    "    def __init__(self, data_storage):\n",
    "        self.data_storage = data_storage\n",
    "        self.estonian_holidays = list(holidays.country_holidays('EE', years=range(2021, 2026)).keys())\n",
    "        \n",
    "    def _add_general_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
    "                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
    "                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "                pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.concat_str(\n",
    "                    \"county\",\n",
    "                    \"is_business\",\n",
    "                    \"product_type\",\n",
    "                    \"is_consumption\",\n",
    "                    separator=\"_\",\n",
    "                ).alias(\"segment\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "    def _add_client_features(self, df_features):\n",
    "        df_client = self.data_storage.df_client\n",
    "\n",
    "        df_features = df_features.join(\n",
    "            df_client.with_columns(\n",
    "                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "            ),\n",
    "            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        return df_features\n",
    "        \n",
    "    def _add_forecast_weather_features(self, df_features):\n",
    "        df_forecast_weather = self.data_storage.df_forecast_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "        df_forecast_weather = (\n",
    "            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n",
    "            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n",
    "            .drop(\"hours_ahead\")\n",
    "            .with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "        df_forecast_weather_local = (\n",
    "            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [0, 7 * 24]:\n",
    "            df_forecast_weather_local = df_forecast_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "            )\n",
    "            df_features = df_features.join( #unique list?\n",
    "                df_forecast_weather_local,\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_local_{hours_lag}h\",\n",
    "            )\n",
    "            \n",
    "            \n",
    "        u_comp = df_features['10_metre_u_wind_component']\n",
    "        v_comp = df_features['10_metre_v_wind_component']\n",
    "        for hours_lag in [0, 168]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (np.sqrt(u_comp ** 2 + v_comp ** 2)).alias(f\"windspeed_10m_forecast_local_{hours_lag}h\"),\n",
    "        \n",
    "            )\n",
    "        return df_features\n",
    "    \n",
    "    def _add_historical_weather_features(self, df_features):\n",
    "        df_historical_weather = self.data_storage.df_historical_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_historical_weather = (\n",
    "            df_historical_weather.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "        \n",
    "        df_historical_weather_local = (\n",
    "            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "        \n",
    "        df_historical_weather_local = df_historical_weather_local.with_columns(\n",
    "            (pl.col(\"rain\") / 1000 + pl.col(\"snowfall\") / 100).alias('total_precipitation'))\n",
    "\n",
    "        for hours_lag in [2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_local_{hours_lag}h\",\n",
    "            )\n",
    "        return df_features\n",
    "    \n",
    "    def _rename_columns(self, df_features):\n",
    "        for_list = ['temperature', 'dewpoint',\n",
    "       'cloudcover_high', 'cloudcover_low', 'cloudcover_mid',\n",
    "       'cloudcover_total', '10_metre_u_wind_component',\n",
    "       '10_metre_v_wind_component', 'direct_solar_radiation',\n",
    "       'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
    "        \n",
    "        hist_list = ['rain', 'surface_pressure', 'windspeed_10m',\n",
    "                     'winddirection_10m', 'shortwave_radiation', 'diffuse_radiation']\n",
    "        \n",
    "        for col in for_list:\n",
    "            df_features = df_features.rename({col: f\"{col}_forecast_local_0h\"})\n",
    "            \n",
    "        for col in hist_list:\n",
    "            df_features = df_features.rename({col: f\"{col}_historical_local_48h\"})\n",
    "        return df_features\n",
    "        \n",
    "    def _add_target_features(self, df_features):\n",
    "        df_target = self.data_storage.df_target\n",
    "        df_client = self.data_storage.df_client\n",
    "        \n",
    "        df_target_all_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\")\n",
    "        )\n",
    "\n",
    "        df_target_all_county_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\", \"county\")\n",
    "        )\n",
    "\n",
    "        for hours_lag in [\n",
    "            2 * 24,\n",
    "            3 * 24,\n",
    "            4 * 24,\n",
    "            5 * 24,\n",
    "            6 * 24,\n",
    "            7 * 24,\n",
    "            8 * 24,\n",
    "            9 * 24,\n",
    "            10 * 24,\n",
    "            11 * 24,\n",
    "            12 * 24,\n",
    "            13 * 24,\n",
    "            14 * 24,\n",
    "            365*24, 367*24, 372*24\n",
    "        ]:\n",
    "            df_features = df_features.join(\n",
    "                df_client.with_columns(\n",
    "                    (pl.col(\"date\") + pl.duration(days=int(hours_lag/24))).cast(pl.Date)\n",
    "                ),\n",
    "                on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n",
    "                how=\"left\", suffix=f\"_{hours_lag}h\"\n",
    "            )\n",
    "            \n",
    "            df_features = df_features.join(\n",
    "                df_target.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n",
    "                on=[\n",
    "                    \"county\",\n",
    "                    \"is_business\",\n",
    "                    \"product_type\",\n",
    "                    \"is_consumption\",\n",
    "                    \"datetime\",\n",
    "                ],\n",
    "                how=\"left\",\n",
    "            )\n",
    "#             df_features = df_features.with_columns((\n",
    "#             pl.col(f\"target_{hours_lag}h\") / pl.col(f\"eic_count_{hours_lag}h\")).alias(f\"new_target_{hours_lag}h\"))\n",
    "            \n",
    "        columns_to_calculate_mean = [f'target_{hours}h' for hours in [\n",
    "            2 * 24, 3 * 24, 4 * 24, 5 * 24, 6 * 24, 7 * 24, 8 * 24\n",
    "        ]]\n",
    "        df_features = df_features.with_columns(df_features.select(columns_to_calculate_mean).mean(axis=1).fill_null(strategy='forward').alias('norm_target'))\n",
    "        \n",
    "        for hours_lag in [\n",
    "            2 * 24,\n",
    "            3 * 24,\n",
    "            4 * 24,\n",
    "            5 * 24,\n",
    "            6 * 24,\n",
    "            7 * 24,\n",
    "            8 * 24,\n",
    "            9 * 24,\n",
    "            10 * 24,\n",
    "            11 * 24,\n",
    "            12 * 24,\n",
    "            13 * 24,\n",
    "            14 * 24,\n",
    "            365*24, 367*24, 372*24\n",
    "        ]:\n",
    "            df_features = df_features.with_columns((\n",
    "            pl.col(f\"target_{hours_lag}h\") / pl.col(f\"norm_target\")).alias(f\"new_target_{hours_lag}h\"))\n",
    "        \n",
    "#         df_features = df_features.with_columns((\n",
    "#             pl.col(f\"eic_count\") / pl.col(f\"norm_target\")).alias(f\"eic_count_normalized\"))\n",
    "        \n",
    "\n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_county_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        cols_for_stats = [\n",
    "            f\"new_target_{hours_lag}h\" for hours_lag in [2 * 24,\n",
    "            3 * 24,\n",
    "            4 * 24,\n",
    "            5 * 24,\n",
    "           ]\n",
    "        ]\n",
    "        df_features = df_features.with_columns(\n",
    "            df_features.select(cols_for_stats).mean(axis=1).alias(f\"new_target_mean\"),\n",
    "            df_features.select(cols_for_stats)\n",
    "            .transpose()\n",
    "            .std()\n",
    "            .transpose()\n",
    "            .to_series()\n",
    "            .alias(f\"new_target_std\"),\n",
    "        )\n",
    "\n",
    "        for target_prefix, lag_numerator, lag_denominator in [\n",
    "            (\"new_target\", 24 * 7, 24 * 14),\n",
    "            (\"new_target\", 24 * 2, 24 * 9),\n",
    "            (\"new_target\", 24 * 3, 24 * 10),\n",
    "            (\"new_target\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 7, 24 * 14),\n",
    "            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{target_prefix}_{lag_numerator}h\")\n",
    "                    / (pl.col(f\"{target_prefix}_{lag_denominator}h\") + 1e-3)\n",
    "                ).alias(f\"{target_prefix}_ratio_{lag_numerator}_{lag_denominator}\")\n",
    "            )\n",
    "            \n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{target_prefix}_{lag_numerator}h\")\n",
    "                    - (pl.col(f\"{target_prefix}_{lag_denominator}h\"))\n",
    "                ).alias(f\"{target_prefix}_difference_{lag_numerator}_{lag_denominator}\")\n",
    "            )\n",
    "        return df_features\n",
    "    \n",
    "    def _normalize_target_columns(self, df_features):\n",
    "        columns_to_calculate_mean = [f'target_{hours}h' for hours in [2 * 24, 3 * 24, 4 * 24, 5 * 24, 6 * 24, 7 * 24, 8 * 24]]\n",
    "        df_features = df_features.with_columns(df_features.select(columns_to_calculate_mean).mean(axis=1).fill_null(strategy='forward').alias('norm_target'))\n",
    "        \n",
    "        \n",
    "        \n",
    "        columns_to_norm = [column for column in df_features.columns if ('target' in column and column != 'norm_target') ]\n",
    "        for column in columns_to_norm:\n",
    "            df_features = df_features.with_columns((pl.col(column) / pl.col('norm_target')).alias(f'{column}_normalized'))\n",
    "\n",
    "        df_features = df_features.with_columns((pl.col('eic_count') / pl.col('norm_target')).alias(f'eic_count_normalized'))\n",
    "        \n",
    "        return df_features\n",
    "    \n",
    "    def _add_holidays_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.when(pl.col(\"date\").is_in(self.estonian_holidays)).then(1).otherwise(0).alias(\"country_holiday\"),\n",
    "                pl.when((pl.col(\"date\") - datetime.timedelta(days=7)).is_in(self.estonian_holidays)).then(1).otherwise(0).alias(\"country_holiday_7d\"),\n",
    "                pl.when((pl.col(\"date\") - datetime.timedelta(days=2)).is_in(self.estonian_holidays)).then(1).otherwise(0).alias(\"country_holiday_2d\")\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def _add_imbalance_features(self, df_features):\n",
    "        imb_list_forecast_0 = [\n",
    "                      'dewpoint_forecast_local_0h',\n",
    "                      'temperature_forecast_local_0h',\n",
    "                      'direct_solar_radiation_forecast_local_0h',\n",
    "                      'surface_solar_radiation_downwards_forecast_local_0h',\n",
    "                      'total_precipitation_forecast_local_0h']\n",
    "        imb_list_forecast_168 = [\n",
    "                      'dewpoint_forecast_local_168h',\n",
    "                      'temperature_forecast_local_168h',\n",
    "                      'direct_solar_radiation_forecast_local_168h',\n",
    "                      'surface_solar_radiation_downwards_forecast_local_168h',\n",
    "                      'total_precipitation_forecast_local_168h']\n",
    "        for feature2, feature3 in zip(imb_list_forecast_0, imb_list_forecast_168):\n",
    "            df_features = df_features.with_columns(((pl.col(feature2) - pl.col(feature3)) / (pl.col(feature2) + pl.col(feature3))).cast(pl.Float32).alias(f\"{feature2}_{feature3}_imbalance\"))\n",
    "        return df_features\n",
    "\n",
    "    def _reduce_memory_usage(self, df_features):\n",
    "        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "        return df_features\n",
    "    \n",
    "    def _drop_columns(self, df_features):\n",
    "        df_features = df_features.drop(\n",
    "            \"date\", \"datetime\", \"hour\", \"dayofyear\", 'block', 'forecast_date', 'eic_count_120h','eic_count_144h','eic_count_168h',\n",
    "            'eic_count_192h','eic_count_216h','eic_count_240h','eic_count_264h','eic_count_288h','eic_count_312h','eic_count_336h','eic_count_48h',\n",
    "            'eic_count_72h','eic_count_8760h','eic_count_8808h','eic_count_8928h','eic_count_96h','installed_capacity_120h',\n",
    "            'installed_capacity_144h','installed_capacity_168h','installed_capacity_192h','installed_capacity_216h','installed_capacity_240h','installed_capacity_264h',\n",
    "            'installed_capacity_288h','installed_capacity_312h','installed_capacity_336h','installed_capacity_48h','installed_capacity_72h','installed_capacity_8760h',\n",
    "            'installed_capacity_8808h','installed_capacity_8928h','installed_capacity_96h',\n",
    "            'target_120h','target_144h','target_168h','target_192h','target_216h','target_240h',\n",
    "            'target_264h','target_288h','target_312h','target_336h', 'target_360h,','target_48h','target_72h','target_8760h','target_8808h',\n",
    "            'target_8928h','target_96h','target_all_county_type_sum_168h','target_all_county_type_sum_336h','target_all_county_type_sum_48h','target_all_county_type_sum_72h','target_all_county_type_sum_difference_168_336',\n",
    "            'target_all_county_type_sum_difference_48_72','target_all_county_type_sum_ratio_168_336','target_all_county_type_sum_ratio_48_72','target_all_type_sum_168h','target_all_type_sum_336h','target_all_type_sum_48h',\n",
    "            'target_all_type_sum_72h','target_all_type_sum_difference_168_336','target_all_type_sum_difference_48_72','target_all_type_sum_ratio_168_336','target_all_type_sum_ratio_48_72',\n",
    "            'installed_capacity', 'day', '10_metre_u_wind_component_forecast_local_0h', '10_metre_v_wind_component_forecast_local_0h',\n",
    "            '10_metre_u_wind_component_forecast_local_168h', '10_metre_v_wind_component_forecast_local_168h'\n",
    "            'winddirection_10m_historical_local_168h', 'winddirection_10m_historical_local_48h', #???\n",
    "            'cloudcover_high_forecast_local_0h',\n",
    "            'cloudcover_high_forecast_local_168h',\n",
    "            'cloudcover_high_historical_local_168h',\n",
    "            'cloudcover_high_historical_local_48h',\n",
    "            'cloudcover_low_forecast_local_0h',\n",
    "            'cloudcover_low_forecast_local_168h',\n",
    "            'cloudcover_low_historical_local_168h',\n",
    "            'cloudcover_low_historical_local_48h',\n",
    "            'cloudcover_mid_forecast_local_0h',\n",
    "            'cloudcover_mid_forecast_local_168h',\n",
    "            'cloudcover_mid_historical_local_168h',\n",
    "            'cloudcover_mid_historical_local_48h',\n",
    "            'cloudcover_total_forecast_local_0h',\n",
    "            'cloudcover_total_forecast_local_168h',\n",
    "            'cloudcover_total_historical_local_168h',\n",
    "            'cloudcover_total_historical_local_48h',\n",
    "            )\n",
    "        return df_features\n",
    "    \n",
    "    \n",
    "    def _to_pandas(self, df_features, y):\n",
    "        cat_cols = [\n",
    "            \"county\",\n",
    "            \"is_business\",\n",
    "            \"product_type\",\n",
    "            \"is_consumption\",\n",
    "            \"segment\",\n",
    "        ]\n",
    "\n",
    "        if y is not None:\n",
    "            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n",
    "        else:\n",
    "            df_features = df_features.to_pandas()\n",
    "\n",
    "        df_features = df_features.set_index(\"row_id\")\n",
    "        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def generate_features(self, df_prediction_items):\n",
    "        if \"target\" in df_prediction_items.columns:\n",
    "            df_prediction_items, y = (\n",
    "                df_prediction_items.drop(\"target\"),\n",
    "                df_prediction_items.select(\"target\"),\n",
    "            )\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        df_features = df_prediction_items.with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "\n",
    "        for add_features in [\n",
    "            self._add_general_features,\n",
    "            self._add_client_features,\n",
    "            self._add_forecast_weather_features,\n",
    "            #self._add_electricity_and_gas_features,\n",
    "            self._add_historical_weather_features,\n",
    "            self._rename_columns,\n",
    "            self._add_target_features,\n",
    "            #self._normalize_target_columns,\n",
    "            self._add_holidays_features,\n",
    "            #self._add_historical_weather_advanced_features,\n",
    "            #self._add_forecast_ratio_features,\n",
    "            #self._reduce_memory_usage,\n",
    "            #self._select_features,\n",
    "            self._add_imbalance_features,\n",
    "            self._reduce_memory_usage,\n",
    "            self._drop_columns\n",
    "        ]:\n",
    "            df_features = add_features(df_features)\n",
    "\n",
    "        df_features = self._to_pandas(df_features, y)\n",
    "        if y is not None:\n",
    "            df_features['normalized_target'] = df_features['target'] / df_features['norm_target']\n",
    "\n",
    "        return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5fe5ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:58:24.516360Z",
     "iopub.status.busy": "2024-01-31T19:58:24.516005Z",
     "iopub.status.idle": "2024-01-31T19:58:24.582687Z",
     "shell.execute_reply": "2024-01-31T19:58:24.581532Z"
    },
    "papermill": {
     "duration": 0.079011,
     "end_time": "2024-01-31T19:58:24.585355",
     "exception": false,
     "start_time": "2024-01-31T19:58:24.506344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import holidays\n",
    "from itertools import combinations\n",
    "\n",
    "class FeaturesGeneratorProductionNormalized:\n",
    "    def __init__(self, data_storage):\n",
    "        self.data_storage = data_storage\n",
    "        self.estonian_holidays = list(holidays.country_holidays('EE', years=range(2021, 2026)).keys())\n",
    "\n",
    "    def _add_general_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
    "                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
    "                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "                pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.concat_str(\n",
    "                    \"county\",\n",
    "                    \"is_business\",\n",
    "                    \"product_type\",\n",
    "                    \"is_consumption\",\n",
    "                    separator=\"_\",\n",
    "                ).alias(\"segment\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _add_client_features(self, df_features):\n",
    "        df_client = self.data_storage.df_client\n",
    "\n",
    "        df_features = df_features.join(\n",
    "            df_client.with_columns(\n",
    "                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "            ),\n",
    "            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _add_forecast_weather_features(self, df_features):\n",
    "        df_forecast_weather = self.data_storage.df_forecast_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_forecast_weather = (\n",
    "            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n",
    "            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n",
    "            .drop(\"hours_ahead\")\n",
    "            .with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "        \n",
    "        df_forecast_weather_date = (\n",
    "            df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_forecast_weather_local = (\n",
    "            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [0, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "        \n",
    "    \n",
    "    def _add_historical_weather_features(self, df_features):\n",
    "        df_historical_weather = self.data_storage.df_historical_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_historical_weather = (\n",
    "            df_historical_weather.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_date = (\n",
    "            df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_local = (\n",
    "            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [1 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n",
    "                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                )\n",
    "                .filter(pl.col(\"hour\") <= 10)\n",
    "                .drop(\"hour\"),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "        \n",
    "    def _add_forecast_ratio_features(self, df_features):\n",
    "        u_comp = df_features['10_metre_u_wind_component']\n",
    "        v_comp = df_features['10_metre_v_wind_component']\n",
    "        hours_lag = 0\n",
    "        df_features = df_features.with_columns(\n",
    "            (np.sqrt(u_comp ** 2 + v_comp ** 2)).alias(f\"windspeed_10m_forecast_local_{hours_lag}h\"),\n",
    "        \n",
    "        )\n",
    "        angles = np.angle(v_comp.to_numpy()*1j + u_comp.to_numpy(), deg=True)\n",
    "        df_features = df_features.with_columns(\n",
    "            pl.Series(angles).alias(\"winddirection_10m_forecast_local_0h\")\n",
    "        )\n",
    "    \n",
    "        for (feature, lag_numerator, lag_denominator) in [('windspeed_10m', 0, 24 * 2),\n",
    "                                                          ('winddirection_10m', 0, 24 * 2),\n",
    "                                                         ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{feature}_forecast_local_{lag_numerator}h\")\n",
    "                    / (pl.col(f\"{feature}_historical_local_{lag_denominator}h\") + 1e-3)\n",
    "                ).alias(f\"{feature}_forecast_to_hist_ratio_{lag_numerator}_{lag_denominator}\")\n",
    "            )\n",
    "            '''\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{feature}_forecast_local_{lag_numerator}h\")\n",
    "                    - (pl.col(f\"{feature}_historical_local_{lag_denominator}h\"))\n",
    "                ).alias(f\"{feature}_forecast_to_hist_difference_{lag_numerator}_{lag_denominator}\")\n",
    "            )'''\n",
    "        print('ok')\n",
    "        return df_features\n",
    "\n",
    "        \n",
    "\n",
    "    def _add_target_features(self, df_features):\n",
    "        df_target = self.data_storage.df_target\n",
    "        '''\n",
    "        df_target_all_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\")\n",
    "        )\n",
    "\n",
    "        df_target_all_county_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\", \"county\")\n",
    "        )'''\n",
    "        \n",
    "        df_client = self.data_storage.df_client\n",
    "\n",
    "        for hours_lag in [\n",
    "            2 * 24,\n",
    "            3 * 24,\n",
    "            4 * 24,\n",
    "            5 * 24,\n",
    "            6 * 24,\n",
    "            7 * 24,\n",
    "            8 * 24,\n",
    "            9 * 24,\n",
    "            10 * 24,\n",
    "            11 * 24,\n",
    "            12 * 24,\n",
    "            13 * 24,\n",
    "            14 * 24,\n",
    "        ]:\n",
    "            \n",
    "            df_features = df_features.join(\n",
    "            df_client.with_columns(\n",
    "                (pl.col(\"date\") + pl.duration(days=int(hours_lag/24))).cast(pl.Date)\n",
    "            ),\n",
    "            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n",
    "            how=\"left\", suffix=f\"_{hours_lag}h\"\n",
    "        )\n",
    "            \n",
    "            df_features = df_features.join(\n",
    "                df_target.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n",
    "                on=[\n",
    "                    \"county\",\n",
    "                    \"is_business\",\n",
    "                    \"product_type\",\n",
    "                    \"is_consumption\",\n",
    "                    \"datetime\",\n",
    "                ],\n",
    "                how=\"left\",\n",
    "            )\n",
    "            df_features = df_features.with_columns((\n",
    "            pl.col(f\"target_{hours_lag}h\") / pl.col(f\"installed_capacity_{hours_lag}h\")).alias(f\"new_target_{hours_lag}h\"))\n",
    "        '''\n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_county_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n",
    "            )'''\n",
    "\n",
    "        cols_for_stats = [\n",
    "            f\"new_target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n",
    "        ]\n",
    "        df_features = df_features.with_columns(\n",
    "            #df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n",
    "            df_features.select(cols_for_stats).mean_horizontal().alias(f\"new_target_mean\"),\n",
    "            df_features.select(cols_for_stats)\n",
    "            .transpose()\n",
    "            .std()\n",
    "            .transpose()\n",
    "            .to_series()\n",
    "            .alias(f\"new_target_std\"),\n",
    "        )\n",
    "\n",
    "        for target_prefix, lag_numerator, lag_denominator in [\n",
    "            (\"new_target\", 24 * 7, 24 * 14),\n",
    "            (\"new_target\", 24 * 2, 24 * 9),\n",
    "            (\"new_target\", 24 * 3, 24 * 10),\n",
    "            (\"new_target\", 24 * 2, 24 * 3),\n",
    "            #(\"target_all_type_sum\", 24 * 2, 24 * 3),\n",
    "            #(\"target_all_type_sum\", 24 * 7, 24 * 14),\n",
    "            #(\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n",
    "            #(\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{target_prefix}_{lag_numerator}h\")\n",
    "                    / (pl.col(f\"{target_prefix}_{lag_denominator}h\") + 1e-3)\n",
    "                ).alias(f\"{target_prefix}_ratio_{lag_numerator}_{lag_denominator}\")\n",
    "            )\n",
    "            \n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{target_prefix}_{lag_numerator}h\")\n",
    "                    - (pl.col(f\"{target_prefix}_{lag_denominator}h\"))\n",
    "                ).alias(f\"{target_prefix}_difference_{lag_numerator}_{lag_denominator}\")\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "    def _add_imbalance_features(self, df_features):\n",
    "        imb_list = [#'installed_capacity',\n",
    "                      'cloudcover_high',\n",
    "                      'cloudcover_low',\n",
    "                      'direct_solar_radiation',\n",
    "                      'surface_solar_radiation_downwards',\n",
    "                      'total_precipitation',\n",
    "                      'direct_solar_radiation_forecast_local_0h',\n",
    "                      'surface_solar_radiation_downwards_forecast_local_0h',\n",
    "                      'total_precipitation_forecast_local_0h',\n",
    "                      'cloudcover_low_forecast_168h',\n",
    "                      'total_precipitation_forecast_local_168h',\n",
    "                      'cloudcover_mid_historical_local_48h',\n",
    "                      'diffuse_radiation_historical_168h',\n",
    "                      'cloudcover_mid_historical_local_168h',\n",
    "                      'diffuse_radiation_historical_local_168h',\n",
    "                      'new_target_48h', 'new_target_168h']\n",
    "        for feature1 in imb_list:\n",
    "            for feature2 in imb_list:\n",
    "                if feature1 != feature2:\n",
    "                    df_features = df_features.with_columns((pl.col(feature1) / (pl.col(feature2) + 1e-5)).cast(pl.Float32).alias(f\"{feature1}_{feature2}_imbalance\"))\n",
    "        return df_features\n",
    "    \n",
    "    def _add_holidays_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.when(pl.col(\"date\").is_in(self.estonian_holidays)).then(1).otherwise(0).alias(\"country_holiday\"),\n",
    "                pl.when((pl.col(\"date\") - datetime.timedelta(days=7)).is_in(self.estonian_holidays)).then(1).otherwise(0).alias(\"country_holiday_7d\"),\n",
    "                pl.when((pl.col(\"date\") - datetime.timedelta(days=2)).is_in(self.estonian_holidays)).then(1).otherwise(0).alias(\"country_holiday_2d\")\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def _select_features(self, df):\n",
    "        df = df.with_columns(\n",
    "            (pl.col('installed_capacity') * pl.col('surface_solar_radiation_downwards') / (pl.col('temperature') + 273.15)).alias('hz'))\n",
    "        selected_features = ['county', 'year', 'month', 'day', 'is_business', 'product_type',\n",
    "       'weekday', 'segment', 'sin(hour)', 'cos(hour)', 'eic_count','installed_capacity', 'direct_solar_radiation','direct_solar_radiation_forecast_local_0h',\n",
    "       'surface_solar_radiation_downwards_forecast_local_0h','cloudcover_low_forecast_168h','cloudcover_mid_historical_local_168h','diffuse_radiation_historical_local_168h', 'new_target_72h',\n",
    "       'new_target_96h', 'new_target_120h', 'new_target_144h', 'new_target_168h','new_target_192h', 'new_target_216h', 'new_target_288h', 'new_target_336h',\n",
    "       'new_target_mean', 'new_target_std','new_target_difference_168_336', 'new_target_difference_48_216','cloudcover_high_cloudcover_mid_historical_local_48h_imbalance',\n",
    "       'cloudcover_low_total_precipitation_forecast_local_0h_imbalance','cloudcover_low_new_target_48h_imbalance','direct_solar_radiation_cloudcover_low_imbalance',\n",
    "       'direct_solar_radiation_total_precipitation_imbalance','direct_solar_radiation_total_precipitation_forecast_local_0h_imbalance','direct_solar_radiation_cloudcover_low_forecast_168h_imbalance',\n",
    "       'surface_solar_radiation_downwards_cloudcover_high_imbalance','surface_solar_radiation_downwards_cloudcover_low_imbalance',\n",
    "       'surface_solar_radiation_downwards_total_precipitation_imbalance','surface_solar_radiation_downwards_direct_solar_radiation_forecast_local_0h_imbalance','surface_solar_radiation_downwards_total_precipitation_forecast_local_0h_imbalance',\n",
    "       'surface_solar_radiation_downwards_new_target_168h_imbalance','total_precipitation_cloudcover_low_imbalance','total_precipitation_cloudcover_low_forecast_168h_imbalance',\n",
    "       'total_precipitation_new_target_48h_imbalance', #'target_all_county_type_sum_336h', 'target_all_county_type_sum_48h', 'target_all_type_sum_168h',\n",
    "       #'direct_solar_radiation_forecast_local_0h_installed_capacity_imbalance', 'surface_solar_radiation_downwards_installed_capacity_imbalance', 'direct_solar_radiation_installed_capacity_imbalance','cloudcover_low_installed_capacity_imbalance'\n",
    "       'direct_solar_radiation_forecast_local_0h_cloudcover_low_imbalance',\n",
    "       'direct_solar_radiation_forecast_local_0h_total_precipitation_imbalance',\n",
    "       'direct_solar_radiation_forecast_local_0h_total_precipitation_forecast_local_0h_imbalance',\n",
    "       'direct_solar_radiation_forecast_local_0h_cloudcover_low_forecast_168h_imbalance',\n",
    "       'direct_solar_radiation_forecast_local_0h_total_precipitation_forecast_local_168h_imbalance',\n",
    "       'direct_solar_radiation_forecast_local_0h_new_target_48h_imbalance',\n",
    "       'surface_solar_radiation_downwards_forecast_local_0h_total_precipitation_imbalance',\n",
    "       'surface_solar_radiation_downwards_forecast_local_0h_total_precipitation_forecast_local_0h_imbalance',\n",
    "       'surface_solar_radiation_downwards_forecast_local_0h_total_precipitation_forecast_local_168h_imbalance',\n",
    "       #'total_precipitation_forecast_local_0h_installed_capacity_imbalance',\n",
    "       'total_precipitation_forecast_local_0h_surface_solar_radiation_downwards_forecast_local_0h_imbalance',\n",
    "       'total_precipitation_forecast_local_0h_cloudcover_low_forecast_168h_imbalance',\n",
    "       'total_precipitation_forecast_local_0h_cloudcover_mid_historical_local_48h_imbalance',\n",
    "       'total_precipitation_forecast_local_0h_diffuse_radiation_historical_local_168h_imbalance',\n",
    "       'cloudcover_low_forecast_168h_cloudcover_high_imbalance',\n",
    "       'cloudcover_low_forecast_168h_total_precipitation_imbalance',\n",
    "       'cloudcover_low_forecast_168h_total_precipitation_forecast_local_0h_imbalance',\n",
    "       'cloudcover_low_forecast_168h_total_precipitation_forecast_local_168h_imbalance',\n",
    "       'total_precipitation_forecast_local_168h_cloudcover_low_forecast_168h_imbalance',\n",
    "       'total_precipitation_forecast_local_168h_cloudcover_mid_historical_local_168h_imbalance',\n",
    "       'diffuse_radiation_historical_168h_total_precipitation_imbalance',\n",
    "       'diffuse_radiation_historical_168h_surface_solar_radiation_downwards_forecast_local_0h_imbalance',\n",
    "       'diffuse_radiation_historical_168h_new_target_168h_imbalance',\n",
    "       'diffuse_radiation_historical_local_168h_new_target_48h_imbalance',\n",
    "       'new_target_48h_cloudcover_low_imbalance',\n",
    "       'new_target_48h_direct_solar_radiation_imbalance',\n",
    "       'new_target_48h_surface_solar_radiation_downwards_imbalance',\n",
    "       'new_target_48h_total_precipitation_imbalance',\n",
    "       'new_target_48h_total_precipitation_forecast_local_0h_imbalance',\n",
    "       'new_target_48h_total_precipitation_forecast_local_168h_imbalance',\n",
    "       'new_target_48h_cloudcover_mid_historical_local_48h_imbalance',\n",
    "       'new_target_48h_diffuse_radiation_historical_168h_imbalance',\n",
    "       'new_target_48h_diffuse_radiation_historical_local_168h_imbalance',\n",
    "       'new_target_48h_new_target_168h_imbalance',\n",
    "       'new_target_168h_cloudcover_low_imbalance',\n",
    "       'new_target_168h_surface_solar_radiation_downwards_imbalance',\n",
    "       'new_target_168h_total_precipitation_imbalance',\n",
    "       'new_target_168h_total_precipitation_forecast_local_0h_imbalance',\n",
    "       'new_target_168h_total_precipitation_forecast_local_168h_imbalance',\n",
    "       'new_target_168h_diffuse_radiation_historical_168h_imbalance', #'installed_capacity_cloudcover_low_imbalance','installed_capacity_direct_solar_radiation_imbalance',\n",
    "       #'installed_capacity_total_precipitation_imbalance','installed_capacity_direct_solar_radiation_forecast_local_0h_imbalance','installed_capacity_total_precipitation_forecast_local_0h_imbalance',\n",
    "       #'installed_capacity_total_precipitation_forecast_local_168h_imbalance'\n",
    "       'new_target_168h_diffuse_radiation_historical_local_168h_imbalance', 'is_consumption', \"date\",'row_id', 'hz'] #\"datetime\", \"hour\", \"dayofyear\", 'block', 'forecast_date'\n",
    "        intersection = set(selected_features).intersection(df.columns)\n",
    "        \n",
    "        new_df = df[selected_features]\n",
    "        #new_df = df[set(selected_featu]\n",
    "        del df\n",
    "        gc.collect()\n",
    "        return new_df\n",
    "        \n",
    "    def _reduce_memory_usage(self, df_features):\n",
    "        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "        return df_features\n",
    "\n",
    "    def _drop_columns(self, df_features):\n",
    "        df_features = df_features.drop(\n",
    "            \"date\", \"datetime\", \"hour\", \"dayofyear\", 'block', 'forecast_date'\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _to_pandas(self, df_features, y):\n",
    "        cat_cols = [\n",
    "            \"county\",\n",
    "            \"is_business\",\n",
    "            \"product_type\",\n",
    "            \"is_consumption\",\n",
    "            \"segment\",\n",
    "        ]\n",
    "\n",
    "        if y is not None:\n",
    "            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n",
    "        else:\n",
    "            df_features = df_features.to_pandas()\n",
    "\n",
    "        df_features = df_features.set_index(\"row_id\")\n",
    "        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def generate_features(self, df_prediction_items):\n",
    "        if \"target\" in df_prediction_items.columns:\n",
    "            df_prediction_items, y = (\n",
    "                df_prediction_items.drop(\"target\"),\n",
    "                df_prediction_items.select(\"target\"),\n",
    "            )\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        df_features = df_prediction_items.with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "\n",
    "        for add_features in [\n",
    "            self._add_general_features,\n",
    "            self._add_client_features,\n",
    "            self._add_forecast_weather_features,\n",
    "            self._add_historical_weather_features,\n",
    "            self._add_target_features,\n",
    "            self._add_holidays_features,\n",
    "            self._add_forecast_ratio_features,\n",
    "            #self._reduce_memory_usage,\n",
    "            #self._add_imbalance_features,\n",
    "            #self._select_features,\n",
    "            self._reduce_memory_usage,\n",
    "            self._drop_columns,\n",
    "        ]:\n",
    "            df_features = add_features(df_features)\n",
    "\n",
    "        df_features = self._to_pandas(df_features, y)\n",
    "        if y is not None:\n",
    "            df_features['normalized_target'] = df_features['target'] / df_features['installed_capacity']\n",
    "            df_features= df_features.drop(columns=['target'])\n",
    "        return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2993ccb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:58:24.605602Z",
     "iopub.status.busy": "2024-01-31T19:58:24.605245Z",
     "iopub.status.idle": "2024-01-31T19:58:24.673622Z",
     "shell.execute_reply": "2024-01-31T19:58:24.672664Z"
    },
    "papermill": {
     "duration": 0.081755,
     "end_time": "2024-01-31T19:58:24.676113",
     "exception": false,
     "start_time": "2024-01-31T19:58:24.594358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import holidays\n",
    "from itertools import combinations\n",
    "\n",
    "class FeaturesGeneratorProduction:\n",
    "    def __init__(self, data_storage):\n",
    "        self.data_storage = data_storage\n",
    "        self.estonian_holidays = list(holidays.country_holidays('EE', years=range(2021, 2026)).keys())\n",
    "\n",
    "    def _add_general_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
    "                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
    "                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "                pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.concat_str(\n",
    "                    \"county\",\n",
    "                    \"is_business\",\n",
    "                    \"product_type\",\n",
    "                    \"is_consumption\",\n",
    "                    separator=\"_\",\n",
    "                ).alias(\"segment\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _add_client_features(self, df_features):\n",
    "        df_client = self.data_storage.df_client\n",
    "\n",
    "        df_features = df_features.join(\n",
    "            df_client.with_columns(\n",
    "                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "            ),\n",
    "            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _add_forecast_weather_features(self, df_features):\n",
    "        df_forecast_weather = self.data_storage.df_forecast_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_forecast_weather = (\n",
    "            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n",
    "            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n",
    "            .drop(\"hours_ahead\")\n",
    "            .with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "        \n",
    "        df_forecast_weather_date = (\n",
    "            df_forecast_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_forecast_weather_local = (\n",
    "            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [0, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_forecast_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "        \n",
    "    \n",
    "    def _add_historical_weather_features(self, df_features):\n",
    "        df_historical_weather = self.data_storage.df_historical_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_historical_weather = (\n",
    "            df_historical_weather.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_date = (\n",
    "            df_historical_weather.group_by(\"datetime\").mean().drop(\"county\")\n",
    "        )\n",
    "\n",
    "        df_historical_weather_local = (\n",
    "            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [1 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_date.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag),\n",
    "                    pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                )\n",
    "                .filter(pl.col(\"hour\") <= 10)\n",
    "                .drop(\"hour\"),\n",
    "                on=\"datetime\",\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "        \n",
    "    def _add_forecast_ratio_features(self, df_features):\n",
    "        u_comp = df_features['10_metre_u_wind_component']\n",
    "        v_comp = df_features['10_metre_v_wind_component']\n",
    "        hours_lag = 0\n",
    "        df_features = df_features.with_columns(\n",
    "            (np.sqrt(u_comp ** 2 + v_comp ** 2)).alias(f\"windspeed_10m_forecast_local_{hours_lag}h\"),\n",
    "        \n",
    "        )\n",
    "        angles = np.angle(v_comp.to_numpy()*1j + u_comp.to_numpy(), deg=True)\n",
    "        df_features = df_features.with_columns(\n",
    "            pl.Series(angles).alias(\"winddirection_10m_forecast_local_0h\")\n",
    "        )\n",
    "    \n",
    "        for (feature, lag_numerator, lag_denominator) in [('windspeed_10m', 0, 24 * 2),\n",
    "                                                          ('winddirection_10m', 0, 24 * 2),\n",
    "                                                         ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{feature}_forecast_local_{lag_numerator}h\")\n",
    "                    / (pl.col(f\"{feature}_historical_local_{lag_denominator}h\") + 1e-3)\n",
    "                ).alias(f\"{feature}_forecast_to_hist_ratio_{lag_numerator}_{lag_denominator}\")\n",
    "            )\n",
    "            '''\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{feature}_forecast_local_{lag_numerator}h\")\n",
    "                    - (pl.col(f\"{feature}_historical_local_{lag_denominator}h\"))\n",
    "                ).alias(f\"{feature}_forecast_to_hist_difference_{lag_numerator}_{lag_denominator}\")\n",
    "            )'''\n",
    "        print('ok')\n",
    "        return df_features\n",
    "\n",
    "        \n",
    "\n",
    "    def _add_target_features(self, df_features):\n",
    "        df_target = self.data_storage.df_target\n",
    "\n",
    "        df_target_all_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\")\n",
    "        )\n",
    "\n",
    "        df_target_all_county_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\", \"county\")\n",
    "        )\n",
    "\n",
    "        for hours_lag in [\n",
    "            2 * 24,\n",
    "            3 * 24,\n",
    "            4 * 24,\n",
    "            5 * 24,\n",
    "            6 * 24,\n",
    "            7 * 24,\n",
    "            8 * 24,\n",
    "            9 * 24,\n",
    "            10 * 24,\n",
    "            11 * 24,\n",
    "            12 * 24,\n",
    "            13 * 24,\n",
    "            14 * 24,\n",
    "        ]:\n",
    "            df_features = df_features.join(\n",
    "                df_target.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n",
    "                on=[\n",
    "                    \"county\",\n",
    "                    \"is_business\",\n",
    "                    \"product_type\",\n",
    "                    \"is_consumption\",\n",
    "                    \"datetime\",\n",
    "                ],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_county_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        cols_for_stats = [\n",
    "            f\"target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n",
    "        ]\n",
    "        df_features = df_features.with_columns(\n",
    "            #df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n",
    "            df_features.select(cols_for_stats).mean_horizontal().alias(f\"target_mean\"),\n",
    "            df_features.select(cols_for_stats)\n",
    "            .transpose()\n",
    "            .std()\n",
    "            .transpose()\n",
    "            .to_series()\n",
    "            .alias(f\"target_std\"),\n",
    "        )\n",
    "\n",
    "        for target_prefix, lag_numerator, lag_denominator in [\n",
    "            (\"target\", 24 * 7, 24 * 14),\n",
    "            (\"target\", 24 * 2, 24 * 9),\n",
    "            (\"target\", 24 * 3, 24 * 10),\n",
    "            (\"target\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 7, 24 * 14),\n",
    "            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{target_prefix}_{lag_numerator}h\")\n",
    "                    / (pl.col(f\"{target_prefix}_{lag_denominator}h\") + 1e-3)\n",
    "                ).alias(f\"{target_prefix}_ratio_{lag_numerator}_{lag_denominator}\")\n",
    "            )\n",
    "            \n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{target_prefix}_{lag_numerator}h\")\n",
    "                    - (pl.col(f\"{target_prefix}_{lag_denominator}h\"))\n",
    "                ).alias(f\"{target_prefix}_difference_{lag_numerator}_{lag_denominator}\")\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "    def _add_imbalance_features(self, df_features):\n",
    "        imb_list = ['installed_capacity',\n",
    "                      'cloudcover_high',\n",
    "                      'cloudcover_low',\n",
    "                      'direct_solar_radiation',\n",
    "                      'surface_solar_radiation_downwards',\n",
    "                      'total_precipitation',\n",
    "                      'direct_solar_radiation_forecast_local_0h',\n",
    "                      'surface_solar_radiation_downwards_forecast_local_0h',\n",
    "                      'total_precipitation_forecast_local_0h',\n",
    "                      'cloudcover_low_forecast_168h',\n",
    "                      'total_precipitation_forecast_local_168h',\n",
    "                      'cloudcover_mid_historical_local_48h',\n",
    "                      'diffuse_radiation_historical_168h',\n",
    "                      'cloudcover_mid_historical_local_168h',\n",
    "                      'diffuse_radiation_historical_local_168h',\n",
    "                      'target_48h', 'target_168h']\n",
    "        for feature1 in imb_list:\n",
    "            for feature2 in imb_list:\n",
    "                if feature1 != feature2:\n",
    "                    df_features = df_features.with_columns((pl.col(feature1) / (pl.col(feature2) + 1e-5)).cast(pl.Float32).alias(f\"{feature1}_{feature2}_imbalance\"))\n",
    "        return df_features\n",
    "    \n",
    "    def _add_holidays_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.when(pl.col(\"date\").is_in(self.estonian_holidays)).then(1).otherwise(0).alias(\"country_holiday\"),\n",
    "                pl.when((pl.col(\"date\") - datetime.timedelta(days=7)).is_in(self.estonian_holidays)).then(1).otherwise(0).alias(\"country_holiday_7d\"),\n",
    "                pl.when((pl.col(\"date\") - datetime.timedelta(days=2)).is_in(self.estonian_holidays)).then(1).otherwise(0).alias(\"country_holiday_2d\")\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    def _select_features(self, df):\n",
    "        selected_features = ['county', 'year', 'month', 'day', 'is_business', 'product_type',\n",
    "       'weekday', 'segment', 'sin(hour)', 'cos(hour)', 'eic_count','installed_capacity', 'direct_solar_radiation','direct_solar_radiation_forecast_local_0h',\n",
    "       'surface_solar_radiation_downwards_forecast_local_0h','cloudcover_low_forecast_168h','cloudcover_mid_historical_local_168h','diffuse_radiation_historical_local_168h', 'target_72h',\n",
    "       'target_96h', 'target_120h', 'target_144h', 'target_168h','target_192h', 'target_216h', 'target_288h', 'target_336h','target_all_county_type_sum_48h', 'target_all_type_sum_168h',\n",
    "       'target_all_county_type_sum_336h', 'target_mean', 'target_std','target_difference_168_336', 'target_difference_48_216','installed_capacity_cloudcover_low_imbalance','installed_capacity_direct_solar_radiation_imbalance',\n",
    "       'installed_capacity_total_precipitation_imbalance','installed_capacity_direct_solar_radiation_forecast_local_0h_imbalance','installed_capacity_total_precipitation_forecast_local_0h_imbalance',\n",
    "       'installed_capacity_total_precipitation_forecast_local_168h_imbalance','cloudcover_high_cloudcover_mid_historical_local_48h_imbalance','cloudcover_low_installed_capacity_imbalance',\n",
    "       'cloudcover_low_total_precipitation_forecast_local_0h_imbalance','cloudcover_low_target_48h_imbalance','direct_solar_radiation_installed_capacity_imbalance','direct_solar_radiation_cloudcover_low_imbalance',\n",
    "       'direct_solar_radiation_total_precipitation_imbalance','direct_solar_radiation_total_precipitation_forecast_local_0h_imbalance','direct_solar_radiation_cloudcover_low_forecast_168h_imbalance',\n",
    "       'surface_solar_radiation_downwards_installed_capacity_imbalance','surface_solar_radiation_downwards_cloudcover_high_imbalance','surface_solar_radiation_downwards_cloudcover_low_imbalance',\n",
    "       'surface_solar_radiation_downwards_total_precipitation_imbalance','surface_solar_radiation_downwards_direct_solar_radiation_forecast_local_0h_imbalance','surface_solar_radiation_downwards_total_precipitation_forecast_local_0h_imbalance',\n",
    "       'surface_solar_radiation_downwards_target_168h_imbalance','total_precipitation_cloudcover_low_imbalance','total_precipitation_cloudcover_low_forecast_168h_imbalance',\n",
    "       'total_precipitation_target_48h_imbalance',\n",
    "       'direct_solar_radiation_forecast_local_0h_installed_capacity_imbalance',\n",
    "       'direct_solar_radiation_forecast_local_0h_cloudcover_low_imbalance',\n",
    "       'direct_solar_radiation_forecast_local_0h_total_precipitation_imbalance',\n",
    "       'direct_solar_radiation_forecast_local_0h_total_precipitation_forecast_local_0h_imbalance',\n",
    "       'direct_solar_radiation_forecast_local_0h_cloudcover_low_forecast_168h_imbalance',\n",
    "       'direct_solar_radiation_forecast_local_0h_total_precipitation_forecast_local_168h_imbalance',\n",
    "       'direct_solar_radiation_forecast_local_0h_target_48h_imbalance',\n",
    "       'surface_solar_radiation_downwards_forecast_local_0h_total_precipitation_imbalance',\n",
    "       'surface_solar_radiation_downwards_forecast_local_0h_total_precipitation_forecast_local_0h_imbalance',\n",
    "       'surface_solar_radiation_downwards_forecast_local_0h_total_precipitation_forecast_local_168h_imbalance',\n",
    "       'total_precipitation_forecast_local_0h_installed_capacity_imbalance',\n",
    "       'total_precipitation_forecast_local_0h_surface_solar_radiation_downwards_forecast_local_0h_imbalance',\n",
    "       'total_precipitation_forecast_local_0h_cloudcover_low_forecast_168h_imbalance',\n",
    "       'total_precipitation_forecast_local_0h_cloudcover_mid_historical_local_48h_imbalance',\n",
    "       'total_precipitation_forecast_local_0h_diffuse_radiation_historical_local_168h_imbalance',\n",
    "       'cloudcover_low_forecast_168h_cloudcover_high_imbalance',\n",
    "       'cloudcover_low_forecast_168h_total_precipitation_imbalance',\n",
    "       'cloudcover_low_forecast_168h_total_precipitation_forecast_local_0h_imbalance',\n",
    "       'cloudcover_low_forecast_168h_total_precipitation_forecast_local_168h_imbalance',\n",
    "       'total_precipitation_forecast_local_168h_cloudcover_low_forecast_168h_imbalance',\n",
    "       'total_precipitation_forecast_local_168h_cloudcover_mid_historical_local_168h_imbalance',\n",
    "       'diffuse_radiation_historical_168h_total_precipitation_imbalance',\n",
    "       'diffuse_radiation_historical_168h_surface_solar_radiation_downwards_forecast_local_0h_imbalance',\n",
    "       'diffuse_radiation_historical_168h_target_168h_imbalance',\n",
    "       'diffuse_radiation_historical_local_168h_target_48h_imbalance',\n",
    "       'target_48h_cloudcover_low_imbalance',\n",
    "       'target_48h_direct_solar_radiation_imbalance',\n",
    "       'target_48h_surface_solar_radiation_downwards_imbalance',\n",
    "       'target_48h_total_precipitation_imbalance',\n",
    "       'target_48h_total_precipitation_forecast_local_0h_imbalance',\n",
    "       'target_48h_total_precipitation_forecast_local_168h_imbalance',\n",
    "       'target_48h_cloudcover_mid_historical_local_48h_imbalance',\n",
    "       'target_48h_diffuse_radiation_historical_168h_imbalance',\n",
    "       'target_48h_diffuse_radiation_historical_local_168h_imbalance',\n",
    "       'target_48h_target_168h_imbalance',\n",
    "       'target_168h_cloudcover_low_imbalance',\n",
    "       'target_168h_surface_solar_radiation_downwards_imbalance',\n",
    "       'target_168h_total_precipitation_imbalance',\n",
    "       'target_168h_total_precipitation_forecast_local_0h_imbalance',\n",
    "       'target_168h_total_precipitation_forecast_local_168h_imbalance',\n",
    "       'target_168h_diffuse_radiation_historical_168h_imbalance',\n",
    "       'target_168h_diffuse_radiation_historical_local_168h_imbalance', 'is_consumption', \"date\",'row_id'] #\"datetime\", \"hour\", \"dayofyear\", 'block', 'forecast_date'\n",
    "        intersection = set(selected_features).intersection(df.columns)\n",
    "        \n",
    "        new_df = df[selected_features]\n",
    "        #new_df = df[set(selected_featu]\n",
    "        del df\n",
    "        gc.collect()\n",
    "        return new_df\n",
    "        \n",
    "    def _reduce_memory_usage(self, df_features):\n",
    "        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "        return df_features\n",
    "\n",
    "    def _drop_columns(self, df_features):\n",
    "        df_features = df_features.drop(\n",
    "            \"date\", \"datetime\", \"hour\", \"dayofyear\", 'block', 'forecast_date'\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _to_pandas(self, df_features, y):\n",
    "        cat_cols = [\n",
    "            \"county\",\n",
    "            \"is_business\",\n",
    "            \"product_type\",\n",
    "            \"is_consumption\",\n",
    "            \"segment\",\n",
    "        ]\n",
    "\n",
    "        if y is not None:\n",
    "            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n",
    "        else:\n",
    "            df_features = df_features.to_pandas()\n",
    "\n",
    "        df_features = df_features.set_index(\"row_id\")\n",
    "        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def generate_features(self, df_prediction_items):\n",
    "        if \"target\" in df_prediction_items.columns:\n",
    "            df_prediction_items, y = (\n",
    "                df_prediction_items.drop(\"target\"),\n",
    "                df_prediction_items.select(\"target\"),\n",
    "            )\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        df_features = df_prediction_items.with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "\n",
    "        for add_features in [\n",
    "            self._add_general_features,\n",
    "            self._add_client_features,\n",
    "            self._add_forecast_weather_features,\n",
    "            self._add_historical_weather_features,\n",
    "            self._add_target_features,\n",
    "            self._add_holidays_features,\n",
    "            self._add_forecast_ratio_features,\n",
    "            self._reduce_memory_usage,\n",
    "            #self._add_imbalance_features,\n",
    "            #self._select_features,\n",
    "            #self._reduce_memory_usage,\n",
    "            self._drop_columns,\n",
    "        ]:\n",
    "            df_features = add_features(df_features)\n",
    "\n",
    "        df_features = self._to_pandas(df_features, y)\n",
    "\n",
    "        return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e26189b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:58:24.695734Z",
     "iopub.status.busy": "2024-01-31T19:58:24.695095Z",
     "iopub.status.idle": "2024-01-31T19:58:24.748879Z",
     "shell.execute_reply": "2024-01-31T19:58:24.747805Z"
    },
    "papermill": {
     "duration": 0.066287,
     "end_time": "2024-01-31T19:58:24.751476",
     "exception": false,
     "start_time": "2024-01-31T19:58:24.685189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import holidays\n",
    "class FeaturesGeneratorConsumption:\n",
    "    def __init__(self, data_storage):\n",
    "        self.data_storage = data_storage\n",
    "        self.estonian_holidays = list(holidays.country_holidays('EE', years=range(2021, 2026)).keys())\n",
    "        \n",
    "    def _add_general_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.col(\"datetime\").dt.ordinal_day().alias(\"dayofyear\"),\n",
    "                pl.col(\"datetime\").dt.hour().alias(\"hour\"),\n",
    "                pl.col(\"datetime\").dt.day().alias(\"day\"),\n",
    "                pl.col(\"datetime\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"datetime\").dt.month().alias(\"month\"),\n",
    "                pl.col(\"datetime\").dt.year().alias(\"year\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.concat_str(\n",
    "                    \"county\",\n",
    "                    \"is_business\",\n",
    "                    \"product_type\",\n",
    "                    \"is_consumption\",\n",
    "                    separator=\"_\",\n",
    "                ).alias(\"segment\"),\n",
    "            )\n",
    "            .with_columns(\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).sin().alias(\"sin(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"dayofyear\") / 183).cos().alias(\"cos(dayofyear)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).sin().alias(\"sin(hour)\"),\n",
    "                (np.pi * pl.col(\"hour\") / 12).cos().alias(\"cos(hour)\"),\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "    def _add_client_features(self, df_features):\n",
    "        df_client = self.data_storage.df_client\n",
    "\n",
    "        df_features = df_features.join(\n",
    "            df_client.with_columns(\n",
    "                (pl.col(\"date\") + pl.duration(days=2)).cast(pl.Date)\n",
    "            ),\n",
    "            on=[\"county\", \"is_business\", \"product_type\", \"date\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "        return df_features\n",
    "        \n",
    "    def _add_forecast_weather_features(self, df_features):\n",
    "        df_forecast_weather = self.data_storage.df_forecast_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "        df_forecast_weather = (\n",
    "            df_forecast_weather.rename({\"forecast_datetime\": \"datetime\"})\n",
    "            .filter((pl.col(\"hours_ahead\") >= 22) & pl.col(\"hours_ahead\") <= 45)\n",
    "            .drop(\"hours_ahead\")\n",
    "            .with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "        df_forecast_weather_local = (\n",
    "            df_forecast_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "\n",
    "        for hours_lag in [0, 7 * 24]:\n",
    "            df_forecast_weather_local = df_forecast_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "            )\n",
    "            df_features = df_features.join( #unique list?\n",
    "                df_forecast_weather_local,\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_forecast_local_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "    def _add_historical_weather_features(self, df_features):\n",
    "        df_historical_weather = self.data_storage.df_historical_weather\n",
    "        df_weather_station_to_county_mapping = (\n",
    "            self.data_storage.df_weather_station_to_county_mapping\n",
    "        )\n",
    "\n",
    "        df_historical_weather = (\n",
    "            df_historical_weather.with_columns(\n",
    "                pl.col(\"latitude\").cast(pl.datatypes.Float32),\n",
    "                pl.col(\"longitude\").cast(pl.datatypes.Float32),\n",
    "            )\n",
    "            .join(\n",
    "                df_weather_station_to_county_mapping,\n",
    "                how=\"left\",\n",
    "                on=[\"longitude\", \"latitude\"],\n",
    "            )\n",
    "            .drop(\"longitude\", \"latitude\")\n",
    "        )\n",
    "        \n",
    "        df_historical_weather_local = (\n",
    "            df_historical_weather.filter(pl.col(\"county\").is_not_null())\n",
    "            .group_by(\"county\", \"datetime\")\n",
    "            .mean()\n",
    "        )\n",
    "        \n",
    "        df_historical_weather_local = df_historical_weather_local.with_columns(\n",
    "            (pl.col(\"rain\") / 1000 + pl.col(\"snowfall\") / 100).alias('total_precipitation'))\n",
    "\n",
    "        for hours_lag in [2 * 24, 7 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_historical_weather_local.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ),\n",
    "                on=[\"county\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_historical_local_{hours_lag}h\",\n",
    "            )\n",
    "        return df_features\n",
    "    \n",
    "    def _add_forecast_ratio_features(self, df_features):\n",
    "        u_comp = df_features['10_metre_u_wind_component_forecast_local_0h']\n",
    "        v_comp = df_features['10_metre_v_wind_component_forecast_local_0h']\n",
    "        hours_lag = 0\n",
    "        df_features = df_features.with_columns(\n",
    "            (np.sqrt(u_comp ** 2 + v_comp ** 2)).alias(f\"windspeed_10m_forecast_local_{hours_lag}h\"),\n",
    "        \n",
    "        )\n",
    "        angles = np.angle(v_comp.to_numpy()*1j + u_comp.to_numpy(), deg=True)\n",
    "        df_features = df_features.with_columns(\n",
    "            pl.Series(angles).alias(\"winddirection_10m_forecast_local_0h\")\n",
    "        )\n",
    "    \n",
    "        for (feature, lag_numerator, lag_denominator) in [('windspeed_10m', 0, 24 * 2),\n",
    "                                                          ('winddirection_10m', 0, 24 * 2),\n",
    "                                                         ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{feature}_forecast_local_{lag_numerator}h\")\n",
    "                    / (pl.col(f\"{feature}_historical_local_{lag_denominator}h\") + 1e-3)\n",
    "                ).alias(f\"{feature}_forecast_to_hist_ratio_{lag_numerator}_{lag_denominator}\")\n",
    "            )\n",
    "            '''\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{feature}_forecast_local_{lag_numerator}h\")\n",
    "                    - (pl.col(f\"{feature}_historical_local_{lag_denominator}h\"))\n",
    "                ).alias(f\"{feature}_forecast_to_hist_difference_{lag_numerator}_{lag_denominator}\")\n",
    "            )'''\n",
    "        return df_features\n",
    "    \n",
    "    def _rename_columns(self, df_features):\n",
    "        for_list = ['temperature', 'dewpoint',\n",
    "       'cloudcover_high', 'cloudcover_low', 'cloudcover_mid',\n",
    "       'cloudcover_total', '10_metre_u_wind_component',\n",
    "       '10_metre_v_wind_component', 'direct_solar_radiation',\n",
    "       'surface_solar_radiation_downwards', 'snowfall', 'total_precipitation']\n",
    "        \n",
    "        hist_list = ['rain', 'surface_pressure', 'windspeed_10m',\n",
    "                     'winddirection_10m', 'shortwave_radiation', 'diffuse_radiation']\n",
    "        \n",
    "        for col in for_list:\n",
    "            df_features = df_features.rename({col: f\"{col}_forecast_local_0h\"})\n",
    "            \n",
    "        for col in hist_list:\n",
    "            df_features = df_features.rename({col: f\"{col}_historical_local_48h\"})\n",
    "        return df_features\n",
    "\n",
    "        \n",
    "    def _add_target_features(self, df_features):\n",
    "        df_target = self.data_storage.df_target\n",
    "\n",
    "        df_target_all_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"county\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\")\n",
    "        )\n",
    "\n",
    "        df_target_all_county_type_sum = (\n",
    "            df_target.group_by([\"datetime\", \"is_business\", \"is_consumption\"])\n",
    "            .sum()\n",
    "            .drop(\"product_type\", \"county\")\n",
    "        )\n",
    "\n",
    "        for hours_lag in [\n",
    "            2 * 24,\n",
    "            3 * 24,\n",
    "            4 * 24,\n",
    "            5 * 24,\n",
    "            6 * 24,\n",
    "            7 * 24,\n",
    "            8 * 24,\n",
    "            9 * 24,\n",
    "            10 * 24,\n",
    "            11 * 24,\n",
    "            12 * 24,\n",
    "            13 * 24,\n",
    "            14 * 24,\n",
    "            365*24, 367*24, 372*24\n",
    "        ]:\n",
    "            df_features = df_features.join(\n",
    "                df_target.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_{hours_lag}h\"}),\n",
    "                on=[\n",
    "                    \"county\",\n",
    "                    \"is_business\",\n",
    "                    \"product_type\",\n",
    "                    \"is_consumption\",\n",
    "                    \"datetime\",\n",
    "                ],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "        for hours_lag in [2 * 24, 3 * 24, 7 * 24, 14 * 24]:\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"county\", \"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "\n",
    "            df_features = df_features.join(\n",
    "                df_target_all_county_type_sum.with_columns(\n",
    "                    pl.col(\"datetime\") + pl.duration(hours=hours_lag)\n",
    "                ).rename({\"target\": f\"target_all_county_type_sum_{hours_lag}h\"}),\n",
    "                on=[\"is_business\", \"is_consumption\", \"datetime\"],\n",
    "                how=\"left\",\n",
    "                suffix=f\"_all_county_type_sum_{hours_lag}h\",\n",
    "            )\n",
    "\n",
    "        cols_for_stats = [\n",
    "            f\"target_{hours_lag}h\" for hours_lag in [2 * 24, 3 * 24, 4 * 24, 5 * 24]\n",
    "        ]\n",
    "        df_features = df_features.with_columns(\n",
    "            df_features.select(cols_for_stats).mean(axis=1).alias(f\"target_mean\"),\n",
    "            df_features.select(cols_for_stats)\n",
    "            .transpose()\n",
    "            .std()\n",
    "            .transpose()\n",
    "            .to_series()\n",
    "            .alias(f\"target_std\"),\n",
    "        )\n",
    "\n",
    "        for target_prefix, lag_numerator, lag_denominator in [\n",
    "            (\"target\", 24 * 7, 24 * 14),\n",
    "            (\"target\", 24 * 2, 24 * 9),\n",
    "            (\"target\", 24 * 3, 24 * 10),\n",
    "            (\"target\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_type_sum\", 24 * 7, 24 * 14),\n",
    "            (\"target_all_county_type_sum\", 24 * 2, 24 * 3),\n",
    "            (\"target_all_county_type_sum\", 24 * 7, 24 * 14),\n",
    "        ]:\n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{target_prefix}_{lag_numerator}h\")\n",
    "                    / (pl.col(f\"{target_prefix}_{lag_denominator}h\") + 1e-3)\n",
    "                ).alias(f\"{target_prefix}_ratio_{lag_numerator}_{lag_denominator}\")\n",
    "            )\n",
    "            \n",
    "            df_features = df_features.with_columns(\n",
    "                (\n",
    "                    pl.col(f\"{target_prefix}_{lag_numerator}h\")\n",
    "                    - (pl.col(f\"{target_prefix}_{lag_denominator}h\"))\n",
    "                ).alias(f\"{target_prefix}_difference_{lag_numerator}_{lag_denominator}\")\n",
    "            )\n",
    "            \n",
    "        df_features = df_features.with_columns(\n",
    "            (pl.col(\"target_48h\") / pl.col(\"eic_count\")).alias(\"target_eic_ratio_48\"),\n",
    "            (pl.col(\"target_168h\") / pl.col(\"eic_count\")).alias(\"target_eic_ratio_168\")\n",
    "        )\n",
    "\n",
    "        return df_features\n",
    "    \n",
    "    def _add_holidays_features(self, df_features):\n",
    "        df_features = (\n",
    "            df_features.with_columns(\n",
    "                pl.when(pl.col(\"date\").is_in(self.estonian_holidays)).then(1).otherwise(0).alias(\"country_holiday\"),\n",
    "                pl.when((pl.col(\"date\") - datetime.timedelta(days=7)).is_in(self.estonian_holidays)).then(1).otherwise(0).alias(\"country_holiday_7d\"),\n",
    "                pl.when((pl.col(\"date\") - datetime.timedelta(days=2)).is_in(self.estonian_holidays)).then(1).otherwise(0).alias(\"country_holiday_2d\")\n",
    "            )\n",
    "        )\n",
    "        return df_features\n",
    "\n",
    "    def _reduce_memory_usage(self, df_features):\n",
    "        df_features = df_features.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "        return df_features\n",
    "    \n",
    "    def _drop_columns(self, df_features):\n",
    "        df_features = df_features.drop(\n",
    "            \"date\", \"datetime\", \"hour\", \"dayofyear\", 'block', 'forecast_date'\n",
    "        )\n",
    "        return df_features\n",
    "    \n",
    "    \n",
    "    def _to_pandas(self, df_features, y):\n",
    "        cat_cols = [\n",
    "            \"county\",\n",
    "            \"is_business\",\n",
    "            \"product_type\",\n",
    "            \"is_consumption\",\n",
    "            \"segment\",\n",
    "        ]\n",
    "\n",
    "        if y is not None:\n",
    "            df_features = pd.concat([df_features.to_pandas(), y.to_pandas()], axis=1)\n",
    "        else:\n",
    "            df_features = df_features.to_pandas()\n",
    "\n",
    "        df_features = df_features.set_index(\"row_id\")\n",
    "        df_features[cat_cols] = df_features[cat_cols].astype(\"category\")\n",
    "\n",
    "        return df_features\n",
    "\n",
    "    def generate_features(self, df_prediction_items):\n",
    "        if \"target\" in df_prediction_items.columns:\n",
    "            df_prediction_items, y = (\n",
    "                df_prediction_items.drop(\"target\"),\n",
    "                df_prediction_items.select(\"target\"),\n",
    "            )\n",
    "        else:\n",
    "            y = None\n",
    "\n",
    "        df_features = df_prediction_items.with_columns(\n",
    "            pl.col(\"datetime\").cast(pl.Date).alias(\"date\"),\n",
    "        )\n",
    "\n",
    "        for add_features in [\n",
    "            self._add_general_features,\n",
    "            self._add_client_features,\n",
    "            self._add_forecast_weather_features,\n",
    "            self._add_historical_weather_features,\n",
    "            self._rename_columns,\n",
    "            self._add_forecast_ratio_features,\n",
    "            self._add_target_features,\n",
    "            self._add_holidays_features,\n",
    "            self._reduce_memory_usage,\n",
    "            self._drop_columns\n",
    "        ]:\n",
    "            df_features = add_features(df_features)\n",
    "\n",
    "        df_features = self._to_pandas(df_features, y)\n",
    "        #df_features = self._add_holidays_as_binary_features(df_features)\n",
    "\n",
    "        return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8388d2bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:58:24.770147Z",
     "iopub.status.busy": "2024-01-31T19:58:24.769768Z",
     "iopub.status.idle": "2024-01-31T19:59:39.092980Z",
     "shell.execute_reply": "2024-01-31T19:59:39.091564Z"
    },
    "papermill": {
     "duration": 74.33569,
     "end_time": "2024-01-31T19:59:39.095868",
     "exception": false,
     "start_time": "2024-01-31T19:58:24.760178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "cons_storage = DataStorage()\n",
    "consumption_mask = cons_storage.df_data.select(pl.col('is_consumption')).cast(pl.Boolean).to_numpy().flatten()\n",
    "cons_storage.df_data = cons_storage.df_data.filter(consumption_mask)\n",
    "prod_storage = DataStorage()\n",
    "prod_storage.df_data = prod_storage.df_data.filter(~consumption_mask)\n",
    "\n",
    "features_generator_prod = FeaturesGeneratorProduction(data_storage=prod_storage)\n",
    "features_generator_prod_normalized = FeaturesGeneratorProductionNormalized(data_storage=prod_storage)\n",
    "features_generator_cons = FeaturesGeneratorConsumption(data_storage=cons_storage)\n",
    "features_generator_cons_normalized = FeaturesGeneratorConsumptionNormalized(data_storage=cons_storage)\n",
    "df_train_features_prod = features_generator_prod.generate_features(prod_storage.df_data)\n",
    "df_train_features_cons = features_generator_cons.generate_features(cons_storage.df_data)\n",
    "df_train_features_prod_normalized = features_generator_prod_normalized.generate_features(prod_storage.df_data)\n",
    "df_train_features_cons_normalized = features_generator_cons_normalized.generate_features(cons_storage.df_data)\n",
    "\n",
    "df_train_features_prod = df_train_features_prod[df_train_features_prod['target'].notnull()]\n",
    "df_train_features_cons = df_train_features_cons[df_train_features_cons['target'].notnull()]\n",
    "df_train_features_prod_normalized = df_train_features_prod_normalized[df_train_features_prod_normalized['normalized_target'].notnull()]\n",
    "df_train_features_cons_normalized = df_train_features_cons_normalized[df_train_features_cons_normalized['normalized_target'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d245332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:59:39.115775Z",
     "iopub.status.busy": "2024-01-31T19:59:39.115349Z",
     "iopub.status.idle": "2024-01-31T19:59:39.127267Z",
     "shell.execute_reply": "2024-01-31T19:59:39.126262Z"
    },
    "papermill": {
     "duration": 0.024615,
     "end_time": "2024-01-31T19:59:39.129635",
     "exception": false,
     "start_time": "2024-01-31T19:59:39.105020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "class ProdNormalizedModel:\n",
    "    def __init__(self, random_state=0):\n",
    "        self.model_parameters = {\n",
    "            \"n_estimators\": 3000,\n",
    "            \"learning_rate\": 0.01,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "            \"colsample_bynode\": 0.6,\n",
    "            #'use_best_model': True,\n",
    "            #\"lambda\": 3,\n",
    "            \"max_depth\": 9,\n",
    "            \"num_leaves\": 143,\n",
    "            \"min_data_in_leaf\": 55,\n",
    "            \"objective\": \"mape\",\n",
    "            \"device\": \"gpu\",\n",
    "            \"metric\": \"mape\",\n",
    "            #\"early_stopping_round\": 200,\n",
    "            #\"random_state\" : random_state\n",
    "        }\n",
    "        #self.model_production = lgb.LGBMRegressor(**self.model_parameters, random_state=0)\n",
    "        \n",
    "        self.model_production = VotingRegressor(\n",
    "            [\n",
    "                (\n",
    "                    f\"prod_lgb_{i}\",\n",
    "                    lgb.LGBMRegressor(**self.model_parameters, random_state=i),\n",
    "                )\n",
    "                for i in range(10)\n",
    "            ])\n",
    "\n",
    "    def fit(self, df_train, df_val=None):\n",
    "        if df_val is not None:\n",
    "            self.model_production.fit(\n",
    "                X=df_train.drop(columns=['normalized_target', \"is_consumption\", \"segment\"]),\n",
    "                y=df_train[\"normalized_target\"], eval_set = (df_val.drop(columns=['normalized_target', \"is_consumption\", \"segment\"]), df_val[\"normalized_target\"]))\n",
    "            \n",
    "        else:\n",
    "            self.model_production.fit(\n",
    "                X=df_train.drop(columns=['normalized_target', \"is_consumption\", \"segment\"]),\n",
    "                y=df_train[\"normalized_target\"]\n",
    "            )\n",
    "            \n",
    "\n",
    "    def predict(self, df_features):\n",
    "        predictions = self.model_production.predict(df_features.drop(columns=['is_consumption', 'segment'])).clip(0)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9e0bdb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:59:39.149173Z",
     "iopub.status.busy": "2024-01-31T19:59:39.148712Z",
     "iopub.status.idle": "2024-01-31T19:59:39.159808Z",
     "shell.execute_reply": "2024-01-31T19:59:39.158682Z"
    },
    "papermill": {
     "duration": 0.023711,
     "end_time": "2024-01-31T19:59:39.162229",
     "exception": false,
     "start_time": "2024-01-31T19:59:39.138518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "class ProdModel:\n",
    "    def __init__(self, random_state=0):\n",
    "        self.model_parameters = {\n",
    "            \"n_estimators\": 5000,\n",
    "            \"learning_rate\": 0.03,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "            \"colsample_bynode\": 0.6,\n",
    "            #'use_best_model': True,\n",
    "            #\"lambda\": 3,\n",
    "            \"max_depth\": 9,\n",
    "            \"num_leaves\": 143,\n",
    "            \"min_data_in_leaf\": 55,\n",
    "            \"objective\": \"mae\",\n",
    "            \"device\": \"gpu\",\n",
    "            \"metric\": \"mae\",\n",
    "            #\"early_stopping_round\": 200,\n",
    "            #\"random_state\" : random_state\n",
    "        }\n",
    "\n",
    "        self.model_production = VotingRegressor(\n",
    "            [\n",
    "                (\n",
    "                    f\"prod_lgb_{i}\",\n",
    "                    lgb.LGBMRegressor(**self.model_parameters, random_state=i),\n",
    "                )\n",
    "                for i in range(3) #10\n",
    "            ])\n",
    "\n",
    "    def fit(self, df_train, df_val=None):\n",
    "        if df_val is not None:\n",
    "            self.model_production.fit(\n",
    "                X=df_train.drop(columns=['target', \"is_consumption\", \"segment\"]),\n",
    "                y=df_train[\"target\"])#, eval_set = (df_val.drop(columns=['target', \"is_consumption\", \"segment\"]), df_val[\"target\"])\n",
    "            \n",
    "        else:\n",
    "            self.model_production.fit(\n",
    "                X=df_train.drop(columns=['target', \"is_consumption\", \"segment\"]),\n",
    "                y=df_train[\"target\"]\n",
    "            )\n",
    "            \n",
    "\n",
    "    def predict(self, df_features):\n",
    "        predictions = self.model_production.predict(df_features.drop(columns=['is_consumption', 'segment'])).clip(0)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "363b93fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:59:39.182852Z",
     "iopub.status.busy": "2024-01-31T19:59:39.181904Z",
     "iopub.status.idle": "2024-01-31T19:59:39.196107Z",
     "shell.execute_reply": "2024-01-31T19:59:39.194951Z"
    },
    "papermill": {
     "duration": 0.027218,
     "end_time": "2024-01-31T19:59:39.198593",
     "exception": false,
     "start_time": "2024-01-31T19:59:39.171375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConsBusiModel:\n",
    "    def __init__(self, random_state=0):\n",
    "        self.model_parameters = {\n",
    "            \"n_estimators\": 3000,\n",
    "            \"learning_rate\": 0.02,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "            \"colsample_bynode\": 0.6,\n",
    "            #'use_best_model': True,\n",
    "            #\"lambda\": 3,\n",
    "            \"max_depth\": 8,\n",
    "            \"num_leaves\": 291,\n",
    "            \"min_data_in_leaf\": 131,\n",
    "            \"objective\": \"mae\",\n",
    "            \"device\": \"gpu\",\n",
    "            \"metric\": \"mae\",\n",
    "            #'early_stopping_rounds': 200, \n",
    "            #\"random_state\" : random_state\n",
    "        }\n",
    "        self.feats = ['county', 'day', 'weekday', 'month', 'sin(dayofyear)',\n",
    "       'cos(dayofyear)', 'sin(hour)', 'cos(hour)', 'eic_count',\n",
    "       'installed_capacity', 'temperature_forecast_local_0h',\n",
    "       'dewpoint_forecast_local_0h', 'cloudcover_low_forecast_local_0h',\n",
    "       '10_metre_u_wind_component_forecast_local_0h',\n",
    "       '10_metre_v_wind_component_forecast_local_0h',\n",
    "       'direct_solar_radiation_forecast_local_0h',\n",
    "       'surface_solar_radiation_downwards_forecast_local_0h',\n",
    "       'snowfall_forecast_local_0h',\n",
    "       'total_precipitation_forecast_local_0h',\n",
    "       'temperature_forecast_local_168h', 'dewpoint_forecast_local_168h',\n",
    "       'direct_solar_radiation_forecast_local_168h',\n",
    "       'surface_solar_radiation_downwards_forecast_local_168h',\n",
    "       'temperature_historical_local_48h',\n",
    "       'dewpoint_historical_local_48h',\n",
    "       'surface_pressure_historical_local_48h',\n",
    "       'winddirection_10m_historical_local_48h',\n",
    "       'shortwave_radiation_historical_local_48h',\n",
    "       'direct_solar_radiation_historical_local_48h',\n",
    "       'temperature_historical_local_168h',\n",
    "       'dewpoint_historical_local_168h',\n",
    "       'surface_pressure_historical_local_168h',\n",
    "       'cloudcover_low_historical_local_168h',\n",
    "       'winddirection_10m_historical_local_168h',\n",
    "       'shortwave_radiation_historical_local_168h',\n",
    "       'direct_solar_radiation_historical_local_168h',\n",
    "       'diffuse_radiation_historical_local_168h', 'target_48h',\n",
    "       'target_72h', 'target_96h', 'target_120h', 'target_144h',\n",
    "       'target_168h', 'target_192h', 'target_216h', 'target_240h',\n",
    "       'target_264h', 'target_288h', 'target_312h', 'target_336h',\n",
    "       'target_8760h', 'target_8928h', 'target_all_county_type_sum_48h',\n",
    "       'target_all_type_sum_72h', 'target_all_county_type_sum_72h',\n",
    "       'target_all_county_type_sum_168h', 'target_all_type_sum_336h',\n",
    "       'target_all_county_type_sum_336h', 'target_mean', 'target_std',\n",
    "       'target_ratio_168_336', 'target_difference_168_336',\n",
    "       'target_ratio_48_216', 'target_difference_48_216',\n",
    "       'target_ratio_72_240', 'target_difference_72_240',\n",
    "       'target_difference_48_72',\n",
    "       'target_all_county_type_sum_ratio_168_336',\n",
    "       'target_all_county_type_sum_difference_168_336',\n",
    "       'target_eic_ratio_48', 'target_eic_ratio_168',\n",
    "       'country_holiday', 'country_holiday_2d', 'country_holiday_7d']\n",
    "        self.model_cons_busi = VotingRegressor(\n",
    "            [\n",
    "                (\n",
    "                    f\"cons_busi_lgb_{i}\",\n",
    "                    lgb.LGBMRegressor(**self.model_parameters, random_state=i),\n",
    "                )\n",
    "                for i in range(3)\n",
    "            ])\n",
    "\n",
    "    def fit(self, df_train, df_val=None):\n",
    "        if df_val is not None:\n",
    "            self.model_cons_busi.fit(\n",
    "                X=df_train[self.feats],\n",
    "                y=df_train[\"target\"])#, eval_set = (df_val[self.feats], df_val[\"target\"])\n",
    "            \n",
    "        else:\n",
    "            self.model_cons_busi.fit(\n",
    "                X=df_train[self.feats],\n",
    "                y=df_train[\"target\"]\n",
    "            )\n",
    "\n",
    "    def predict(self, df_features):\n",
    "        predictions = self.model_cons_busi.predict(df_features[self.feats]).clip(0)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d0332f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:59:39.218640Z",
     "iopub.status.busy": "2024-01-31T19:59:39.218214Z",
     "iopub.status.idle": "2024-01-31T19:59:39.230965Z",
     "shell.execute_reply": "2024-01-31T19:59:39.229576Z"
    },
    "papermill": {
     "duration": 0.025994,
     "end_time": "2024-01-31T19:59:39.233717",
     "exception": false,
     "start_time": "2024-01-31T19:59:39.207723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConsNoBusiModel:\n",
    "    def __init__(self,random_state=0):\n",
    "        self.model_parameters = {\n",
    "            \"n_estimators\": 3000,\n",
    "            \"learning_rate\": 0.06,\n",
    "            \"colsample_bytree\": 0.9,\n",
    "            \"colsample_bynode\": 0.6,\n",
    "            #'use_best_model': True,\n",
    "            #\"lambda\": 3,\n",
    "            \"max_depth\": 10,\n",
    "            \"num_leaves\": 71,\n",
    "            \"min_data_in_leaf\": 200,\n",
    "            \"objective\": \"mae\",\n",
    "            \"device\": \"gpu\",\n",
    "            \"metric\": \"mae\",\n",
    "            #'early_stopping_rounds': 200, \n",
    "            #\"random_state\" : random_state\n",
    "        }\n",
    "        self.model_cons_no_busi = VotingRegressor(\n",
    "            [\n",
    "                (\n",
    "                    f\"cons_no_busi_{i}\",\n",
    "                    lgb.LGBMRegressor(**self.model_parameters, random_state=i),\n",
    "                )\n",
    "                for i in range(3)\n",
    "            ])\n",
    "\n",
    "    def fit(self, df_train, df_val=None):\n",
    "        if df_val is not None:\n",
    "            self.model_cons_no_busi.fit(\n",
    "                X=df_train.drop(columns=['target', 'is_consumption', 'is_business']),\n",
    "                y=df_train[\"target\"])#, eval_set = (df_val.drop(columns=['target', 'is_consumption', 'is_business']), df_val[\"target\"])\n",
    "            \n",
    "        else:\n",
    "            self.model_cons_no_busi.fit(\n",
    "                X=df_train.drop(columns=['target', 'is_consumption', 'is_business']),\n",
    "                y=df_train[\"target\"]\n",
    "            )\n",
    "\n",
    "    def predict(self, df_features):\n",
    "        predictions = self.model_cons_no_busi.predict(df_features.drop(columns=['is_consumption', 'is_business'])).clip(0)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "292e3cd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:59:39.254571Z",
     "iopub.status.busy": "2024-01-31T19:59:39.253516Z",
     "iopub.status.idle": "2024-01-31T19:59:39.265748Z",
     "shell.execute_reply": "2024-01-31T19:59:39.264658Z"
    },
    "papermill": {
     "duration": 0.025164,
     "end_time": "2024-01-31T19:59:39.268216",
     "exception": false,
     "start_time": "2024-01-31T19:59:39.243052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "class ConsBusiNormalizedModel:\n",
    "    def __init__(self):\n",
    "        cat_cols = ['county', 'is_business', 'product_type', 'is_consumption']\n",
    "            \n",
    "        self.model_parameters = {\n",
    "            \"n_estimators\": 5000,\n",
    "            \"learning_rate\": 0.01,#4,\n",
    "            #\"colsample_bytree\": 0.9,\n",
    "            #\"colsample_bynode\": 0.6,\n",
    "            \"max_depth\": 7,\n",
    "            \"random_strength\": 1.2,\n",
    "            \"cat_features\": cat_cols,\n",
    "            #\"num_leaves\": 71,\n",
    "            #\"min_data_in_leaf\": 200,\n",
    "            \"loss_function\": \"MAE\",\n",
    "            \"task_type\": \"GPU\",\n",
    "            \"eval_metric\": \"MAE\",\n",
    "            #\"random_state\" : 7\n",
    "        }\n",
    "\n",
    "\n",
    "        #self.model_consumption = LGBMRegressor(**self.model_parameters)\n",
    "        self.model_consumption = VotingRegressor(\n",
    "            [\n",
    "                (\n",
    "                    f\"cons_busi_lgb_{i}\",\n",
    "                    CatBoostRegressor(**self.model_parameters, random_state=i),\n",
    "                )\n",
    "                for i in range(10)\n",
    "            ])\n",
    "    def fit(self, df_train, df_val=None):\n",
    "        if df_val is not None:\n",
    "            self.model_consumption.fit(\n",
    "                X=df_train.drop(columns=['normalized_target', 'target', 'segment', 'eic_count', 'year', 'norm_target']),\n",
    "                y=df_train[\"normalized_target\"], eval_set = (df_val.drop(columns=['normalized_target', 'target', 'segment', 'eic_count', 'year', 'norm_target']), df_val[\"normalized_target\"])\n",
    "            )\n",
    "        else:\n",
    "            self.model_consumption.fit(\n",
    "                X=df_train.drop(columns=['normalized_target', 'target', 'segment', 'eic_count', 'year', 'norm_target']),\n",
    "                y=df_train[\"normalized_target\"]\n",
    "            )\n",
    "\n",
    "    def predict(self, df_features):\n",
    "        predictions = np.zeros(len(df_features))\n",
    "\n",
    "        predictions = self.model_consumption.predict(\n",
    "            df_features.drop(columns=['segment', 'eic_count', 'year', 'norm_target'])\n",
    "        ).clip(0)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5b22078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:59:39.288329Z",
     "iopub.status.busy": "2024-01-31T19:59:39.287616Z",
     "iopub.status.idle": "2024-01-31T19:59:39.299481Z",
     "shell.execute_reply": "2024-01-31T19:59:39.298452Z"
    },
    "papermill": {
     "duration": 0.024539,
     "end_time": "2024-01-31T19:59:39.301875",
     "exception": false,
     "start_time": "2024-01-31T19:59:39.277336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "class ConsNoBusiNormalizedModel:\n",
    "    def __init__(self):\n",
    "        cat_cols = ['county', 'is_business', 'product_type', 'is_consumption']\n",
    "            \n",
    "        self.model_parameters = {\n",
    "            \"n_estimators\": 5000,\n",
    "            \"learning_rate\": 0.01,#4,\n",
    "            #\"colsample_bytree\": 0.9,\n",
    "            #\"colsample_bynode\": 0.6,\n",
    "            \"max_depth\": 3,\n",
    "            \"random_strength\":3,\n",
    "            \"cat_features\": cat_cols,\n",
    "            #\"num_leaves\": 71,\n",
    "            #\"min_data_in_leaf\": 200,\n",
    "            \"loss_function\": \"MAE\",\n",
    "            \"task_type\": \"GPU\",\n",
    "            \"eval_metric\": \"MAE\",\n",
    "            #\"random_state\" : 7\n",
    "        }\n",
    "\n",
    "\n",
    "        #self.model_consumption = LGBMRegressor(**self.model_parameters)\n",
    "        self.model_consumption = VotingRegressor(\n",
    "            [\n",
    "                (\n",
    "                    f\"cons_busi_lgb_{i}\",\n",
    "                    CatBoostRegressor(**self.model_parameters, random_state=i),\n",
    "                )\n",
    "                for i in range(10)\n",
    "            ])\n",
    "    def fit(self, df_train, df_val=None):\n",
    "        if df_val is not None:\n",
    "            self.model_consumption.fit(\n",
    "                X=df_train.drop(columns=['normalized_target', 'target', 'segment', 'eic_count', 'year', 'norm_target']),\n",
    "                y=df_train[\"normalized_target\"], eval_set = (df_val.drop(columns=['normalized_target', 'target', 'segment', 'eic_count', 'year', 'norm_target']), df_val[\"normalized_target\"])\n",
    "            )\n",
    "        else:\n",
    "            self.model_consumption.fit(\n",
    "                X=df_train.drop(columns=['normalized_target', 'target', 'segment', 'eic_count', 'year', 'norm_target']),\n",
    "                y=df_train[\"normalized_target\"]\n",
    "            )\n",
    "\n",
    "    def predict(self, df_features):\n",
    "        predictions = np.zeros(len(df_features))\n",
    "\n",
    "        predictions = self.model_consumption.predict(\n",
    "            df_features.drop(columns=['segment', 'eic_count', 'year', 'norm_target'])\n",
    "        ).clip(0)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16ef9bc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:59:39.321885Z",
     "iopub.status.busy": "2024-01-31T19:59:39.320653Z",
     "iopub.status.idle": "2024-01-31T19:59:39.327819Z",
     "shell.execute_reply": "2024-01-31T19:59:39.326891Z"
    },
    "papermill": {
     "duration": 0.019318,
     "end_time": "2024-01-31T19:59:39.330157",
     "exception": false,
     "start_time": "2024-01-31T19:59:39.310839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_online(df, mode):\n",
    "    if mode == 'prod':\n",
    "        prod_mask = df['is_consumption'] == 0\n",
    "        train_df = df[prod_mask]\n",
    "    elif mode == 'no_busi':\n",
    "        no_busi_mask = (df['is_business'] == 0) & (df['is_consumption'] == 1)\n",
    "        train_df = df[no_busi_mask]\n",
    "    elif mode == 'busi':\n",
    "        busi_mask = (df['is_business'] == 1) & (df['is_consumption'] == 1)\n",
    "        train_df = df[busi_mask]\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad1db012",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:59:39.349690Z",
     "iopub.status.busy": "2024-01-31T19:59:39.349227Z",
     "iopub.status.idle": "2024-01-31T19:59:41.785570Z",
     "shell.execute_reply": "2024-01-31T19:59:41.784506Z"
    },
    "papermill": {
     "duration": 2.448913,
     "end_time": "2024-01-31T19:59:41.787939",
     "exception": false,
     "start_time": "2024-01-31T19:59:39.339026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12480"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import enefit\n",
    "\n",
    "env = enefit.make_env()\n",
    "iter_test = env.iter_test()\n",
    "'''\n",
    "year = df.year\n",
    "month = df.month\n",
    "month = df.day\n",
    "VAL_MASK = ((year == 2023)& (month == 5) & (day > 27))\n",
    "\n",
    "check_prod = df_train_features_prod[VAL_MASK]\n",
    "check_cons = df_train_features_cons[VAL_MASK]\n",
    "all_df = np.concat(check_prod, check_cons)\n",
    "'''\n",
    "targets = pd.read_csv('/kaggle/input/predict-energy-behavior-of-prosumers/train.csv')\n",
    "targets = targets[pd.to_datetime(targets['datetime']) >= pd.to_datetime(\"2023-05-28\")]\n",
    "#targets=targets.dropna()\n",
    "len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5de8b645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-31T19:59:41.808145Z",
     "iopub.status.busy": "2024-01-31T19:59:41.807770Z",
     "iopub.status.idle": "2024-01-31T19:59:57.871578Z",
     "shell.execute_reply": "2024-01-31T19:59:57.870459Z"
    },
    "papermill": {
     "duration": 16.077068,
     "end_time": "2024-01-31T19:59:57.874093",
     "exception": false,
     "start_time": "2024-01-31T19:59:41.797025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n",
      "3.808190107345581\n",
      "4.149472951889038\n",
      "3.8647372722625732\n",
      "3.8691442012786865\n",
      "3.902517557144165\n"
     ]
    }
   ],
   "source": [
    "counter = 1147\n",
    "preds_all = np.array([])\n",
    "import time\n",
    "start = time.time()\n",
    "for (\n",
    "    df_test, \n",
    "    df_new_target, \n",
    "    df_new_client, \n",
    "    df_new_historical_weather,\n",
    "    df_new_forecast_weather, \n",
    "    df_new_electricity_prices, \n",
    "    df_new_gas_prices,\n",
    "    df_sample_prediction\n",
    ") in iter_test:\n",
    "    start = time.time()\n",
    "    cons_storage.update_with_new_data(\n",
    "            df_new_client=df_new_client,\n",
    "            df_new_forecast_weather=df_new_forecast_weather,\n",
    "            df_new_historical_weather=df_new_historical_weather,\n",
    "            df_new_target=df_new_target[df_new_target['is_consumption'] == 1]\n",
    "    )\n",
    "    \n",
    "    prod_storage.update_with_new_data(\n",
    "            df_new_client=df_new_client,\n",
    "            df_new_forecast_weather=df_new_forecast_weather,\n",
    "            df_new_historical_weather=df_new_historical_weather,\n",
    "            df_new_target=df_new_target[df_new_target['is_consumption'] == 0]\n",
    "    )\n",
    "    \n",
    "    print(time.time() - start)\n",
    "    if df_test['currently_scored'].all() == 0:\n",
    "        df_sample_prediction[\"target\"] = 0\n",
    "        env.predict(df_sample_prediction)\n",
    "        continue\n",
    "        \n",
    "    consumption_mask = (df_test['is_consumption'] == 1)\n",
    "    global_busi_mask = (df_test['is_consumption'] == 1) & (df_test['is_business'] == 1)\n",
    "    global_no_busi_mask = (df_test['is_consumption'] == 1) & (df_test['is_business'] == 0)\n",
    "    \n",
    "    if counter == 1147:\n",
    "        counter = 0\n",
    "        \n",
    "        df_train_features_prod = features_generator_prod.generate_features(prod_storage.df_data)\n",
    "        df_train_features_cons = features_generator_cons.generate_features(cons_storage.df_data)\n",
    "        df_train_features_prod = df_train_features_prod[df_train_features_prod['target'].notnull()]\n",
    "        df_train_features_cons = df_train_features_cons[df_train_features_cons['target'].notnull()]\n",
    "        df_train_features_prod_normalized = features_generator_prod_normalized.generate_features(prod_storage.df_data)\n",
    "        df_train_features_prod_normalized = df_train_features_prod_normalized[df_train_features_prod_normalized['normalized_target'].notnull()]\n",
    "        df_train_features_cons_normalized = features_generator_cons_normalized.generate_features(cons_storage.df_data)\n",
    "        df_train_features_cons_normalized = df_train_features_cons_normalized[df_train_features_cons_normalized['normalized_target'].notnull()]\n",
    "        \n",
    "        train_df = train_online(df_train_features_prod, 'prod')\n",
    "        prod_model = ProdModel()\n",
    "        prod_model.fit(train_df)\n",
    "        \n",
    "        train_df = train_online(df_train_features_prod_normalized, 'prod')\n",
    "        prod_normalized_model = ProdNormalizedModel()\n",
    "        prod_normalized_model.fit(train_df)\n",
    "\n",
    "        train_df = train_online(df_train_features_cons, 'busi')\n",
    "        cons_busi_model = ConsBusiModel()\n",
    "        cons_busi_model.fit(train_df)\n",
    "        \n",
    "        train_df = train_online(df_train_features_cons_normalized, 'busi')\n",
    "        cons_busi_normalized_model = ConsBusiNormalizedModel()\n",
    "        cons_busi_normalized_model.fit(train_df)\n",
    "\n",
    "        train_df = train_online(df_train_features_cons, 'no_busi')\n",
    "        cons_no_busi_model = ConsNoBusiModel()\n",
    "        cons_no_busi_model.fit(train_df)\n",
    "        \n",
    "        train_df = train_online(df_train_features_cons_normalized, 'no_busi')\n",
    "        cons_no_busi_normalized_model = ConsNoBusiNormalizedModel()\n",
    "        cons_no_busi_normalized_model.fit(train_df)\n",
    "\n",
    "    df_test_prod = prod_storage.preprocess_test(df_test[~consumption_mask])\n",
    "    df_test_prod_normalized = prod_storage.preprocess_test(df_test[~consumption_mask])\n",
    "    df_test_cons = cons_storage.preprocess_test(df_test[consumption_mask])\n",
    "    df_test_cons_normalized = cons_storage.preprocess_test(df_test[consumption_mask])\n",
    "\n",
    "    df_test_features_cons = features_generator_cons.generate_features(df_test_cons)\n",
    "    df_test_features_prod = features_generator_prod.generate_features(df_test_prod)\n",
    "    df_test_features_prod_normalized = features_generator_prod_normalized.generate_features(df_test_prod_normalized)\n",
    "    df_test_features_cons_normalized = features_generator_cons_normalized.generate_features(df_test_cons_normalized)\n",
    "    \n",
    "    print(time.time() - start)\n",
    "    busi_mask = (df_test_features_cons_normalized['is_business'] == 1)\n",
    "    prod_normalized_preds = prod_normalized_model.predict(df_test_features_prod_normalized)\n",
    "    prod_normalized_preds = prod_normalized_preds * df_test_features_prod_normalized['installed_capacity']\n",
    "    nans = prod_normalized_preds.isna()\n",
    "    if nans.sum() > 0:\n",
    "        prod_normalized_preds[nans] = prod_model.predict(df_test_features_prod[nans])\n",
    "    cons_busi_normalized_preds = cons_busi_normalized_model.predict(df_test_features_cons_normalized[busi_mask])\n",
    "    cons_busi_normalized_preds = cons_busi_normalized_preds * df_test_features_cons_normalized[busi_mask]['norm_target']\n",
    "    nans = cons_busi_normalized_preds.isna()\n",
    "    if nans.sum() > 0:\n",
    "        cons_busi_normalized_preds[nans] = cons_busi_model.predict(df_test_features_cons[busi_mask & nans])\n",
    "    \n",
    "    cons_no_busi_normalized_preds = cons_no_busi_normalized_model.predict(df_test_features_cons_normalized[~busi_mask])\n",
    "    cons_no_busi_normalized_preds = cons_no_busi_normalized_preds * df_test_features_cons_normalized[~busi_mask]['norm_target']\n",
    "    nans = cons_no_busi_normalized_preds.isna()\n",
    "    if nans.sum() > 0:\n",
    "        cons_no_busi_normalized_preds[nans] = cons_no_busi_model.predict(df_test_features_cons[~busi_mask & nans])\n",
    "    preds = np.zeros(len(df_test))\n",
    "    \n",
    "    preds[~consumption_mask] = prod_normalized_preds\n",
    "    preds[global_no_busi_mask] = cons_no_busi_normalized_preds\n",
    "    preds[global_busi_mask] = cons_busi_normalized_preds\n",
    "    #print(preds)\n",
    "    #preds_all = np.concatenate((preds_all, preds))\n",
    "    #print(time.time() - start)\n",
    "    df_sample_prediction[\"target\"] = preds #preds #model.predict(df_test_features)\n",
    "    counter += 1\n",
    "    env.predict(df_sample_prediction)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30f613",
   "metadata": {
    "papermill": {
     "duration": 0.009342,
     "end_time": "2024-01-31T19:59:57.893487",
     "exception": false,
     "start_time": "2024-01-31T19:59:57.884145",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7292407,
     "sourceId": 57236,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 107.944013,
   "end_time": "2024-01-31T20:00:00.551872",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-31T19:58:12.607859",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
